{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPodpayQBuS2yR8sVeesV/6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/Generative_AI/blob/main/Chatgpt_with_any_Youtube_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVZdc0pRLOLe",
        "outputId": "bf49f909-caba-4ab9-c0cd-948dfc9c6834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU pytube youtube-transcript-api langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU OpenAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCfNXclIOB7r",
        "outputId": "4b035e82-3f7d-48df-b5ad-3abd9563ae4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.1 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ],
      "metadata": {
        "id": "Lxw592YvUIPj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import YoutubeLoader"
      ],
      "metadata": {
        "id": "KK0oKdsDMM9f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if a Youtube video has Subtitle is disabled for this video."
      ],
      "metadata": {
        "id": "uKaYXRdNU1KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  srt = YouTubeTranscriptApi.get_transcripts(\"https://www.youtube.com/watch?v=mBVaf3FnVL8&list=PLATQCFQn9lGe3xJmyb6ZRENcHptqNR5gs\")\n",
        "except:\n",
        "  print(f\"{id} doesn't have a transcript\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cueSG8wBUPlt",
        "outputId": "0b082286-b29a-411c-b507-7798913b49f5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in function id> doesn't have a transcript\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use langchain to get the transcript from youtube video"
      ],
      "metadata": {
        "id": "sUpGJp8VVKo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = YoutubeLoader.from_youtube_channel('https://www.youtube.com/watch?v=mBVaf3FnVL8&list=PLATQCFQn9lGe3xJmyb6ZRENcHptqNR5gs',add_video_info=True)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "pViHavb_UnUc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5iwHxksUslo",
        "outputId": "90da4770-42b6-4c34-a9e9-4f392e36be16"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"and welcome to the scholar online youtube channel\\xa0\\nthe channel that is all about learning you can\\xa0\\xa0 visit our website on https scholar.online you will\\xa0\\nfind more courses on what we teach right here on\\xa0\\xa0 this youtube channel and much much more remember\\xa0\\nto like this video subscribe to our channel and\\xa0\\xa0 turn on notifications so that you'll be notified\\xa0\\nevery time we have new content follow us on\\xa0\\xa0 facebook instagram and twitter in this video we're\\xa0\\ngoing to start a whole new journey on this channel\\xa0\\xa0 we're gonna we've done a lot of web development\\xa0\\nwe've done a lot of python jungle e-commerce stuff\\xa0\\xa0 you know but because this is a channel that is all\\xa0\\nabout learning i'm always going to be trying and\\xa0\\xa0 striving towards teaching you new and interesting\\xa0\\nthings we want this channel to be the one place\\xa0\\xa0 that you want to come to learn all the latest\\xa0\\ntrends and moving things within technology and\\xa0\\xa0 there's nothing that is more trending these days\\xa0\\nif you're on the internet than web at 3.0 okay so\\xa0\\xa0 i've thought about it and i've decided in the next\\xa0\\ncouple of weeks maybe two months or so we're gonna\\xa0\\xa0 start working on some tutorials and i will be\\xa0\\nteaching you some cool interesting stuff that you\\xa0\\xa0 can start building on web a 3.0 or specifically\\xa0\\nwhat is web 3.0 because i'm sure you've heard\\xa0\\xa0 this term being thrown around a couple of\\xa0\\ntimes by a couple of people here and there\\xa0\\xa0 everyone goes this way what is it so basically in\\xa0\\nvery simple terms if somebody asks you this is how\\xa0\\xa0 you answer them web 3.0 is the next iteration of\\xa0\\nthe internet okay we had the internet one we had\\xa0\\xa0 internet two now we are on the internet three okay\\xa0\\nso think about it as um the industrial revolutions\\xa0\\xa0 okay we've had the first industrial revolution\\xa0\\nthe third one this is supposed to be the fourth\\xa0\\xa0 one that we're in with all these new technologies\\xa0\\nweb 3.0 is something similar to that the term web\\xa0\\xa0 3 itself um actually i first came across web3\\xa0\\num in my blockchain development days okay so if\\xa0\\xa0 you've been on this channel you know i do a lot\\xa0\\nof blog web chat blog development courses some\\xa0\\xa0 of them i do halfway here and then i finish them\\xa0\\non my website but um web 3 is used a lot in um\\xa0\\xa0 the blockchain development space there's actually\\xa0\\na whole python library on web3 maybe i'll do um\\xa0\\xa0 something like that when i'm when i'm on you know\\xa0\\nweb development specifically ethereum development\\xa0\\xa0 maybe we'll do something on ethereum tokens and\\xa0\\num you know let me know if you're interested\\xa0\\xa0 in are learning more on um nfts they're the new\\xa0\\nthing today all right they are all over the place\\xa0\\xa0 i mean i was in nfcs back in the days of crypto\\xa0\\nkitties if you still remember crypto kitties the\\xa0\\xa0 very first nfc that came out and it was huge and\\xa0\\ni mean we couldn't believe that we were buying\\xa0\\xa0 these damn cats for like you know like the prices\\xa0\\nthey were buying them for right but um so maybe\\xa0\\xa0 i'll do something on that later but today i don't\\xa0\\nwant to die you know to get out of my mind within\\xa0\\xa0 this journey we will eventually maybe reach that\\xa0\\nspace where we do some development on ethereum\\xa0\\xa0 and i tell you some actual cool stuff that you can\\xa0\\ndevelop right now in your own space with um some\\xa0\\xa0 tokens and maybe other you know cryptocurrency\\xa0\\ntokens you know um the decentralized internet but\\xa0\\xa0 today what i want to focus on is just a high level\\xa0\\ndefinition of web 3.0 what is it okay so web 3.0\\xa0\\xa0 i want to link this in the description of\\xa0\\nthe video that you're watching right now\\xa0\\xa0 i googled it and this is one of the things\\xa0\\nthat come up so if you're defining this to\\xa0\\xa0 um a you know two to a two-year-old it's the\\xa0\\nnext iteration of the internet but specifically\\xa0\\xa0 these are the features or the kind of things\\xa0\\nor the kind of technologies that you should\\xa0\\xa0 be expecting in web 3.0 okay the first one is\\xa0\\ndecentralization i've already mentioned this\\xa0\\xa0 when you're talking about cryptocurrencies and so\\xa0\\nforth that's decentralization so the whole idea\\xa0\\xa0 of um centralization is everything is in\\xa0\\none server if you look at all the things\\xa0\\xa0 we develop all the blocks all the stuff that we\\xa0\\nhave done on this channel it's all centralized\\xa0\\xa0 okay i show you how to do a server that you're\\xa0\\ngonna maybe develop on digitalocean that digital\\xa0\\xa0 ocean server is sitting in one place that's\\xa0\\nbeen controlled by one person i'm the developer\\xa0\\xa0 i control everything that's happening on that\\xa0\\nserver i build it i design it and everybody's\\xa0\\xa0 gonna have to go through my web application to get\\xa0\\nanything done okay so that's web 2.0 that's the\\xa0\\xa0 last that's last year's stuff okay in web 3.0\\xa0\\nwhat are we talking about is decentralization\\xa0\\xa0 stuff is no longer centralized in one place what\\xa0\\nwe're saying is that instead of having one server\\xa0\\xa0 that is serving everything you're going to have a\\xa0\\nnetwork of servers or you're going to have a peer\\xa0\\xa0 peers of servers that are all equal on the same\\xa0\\nlevel and they're able to serve the same content\\xa0\\xa0 or the same products whatever it is you're doing\\xa0\\nso think of cryptocurrency it's is decentralized\\xa0\\xa0 okay that is what made it great okay bitcoin\\xa0\\nthe first cryptocurrency that came about is\\xa0\\xa0 decentralized you no longer have one server that\\xa0\\nis serving the bitcoin network that is why it has\\xa0\\xa0 been so resilient over the years because you can't\\xa0\\nshut it down okay there is millions and millions\\xa0\\xa0 of connected nodes that are running at the\\xa0\\nsoftware okay so if one node goes down there's a\\xa0\\xa0 million more nodes that are that are pumping that\\xa0\\nare sort of working that are pushing that network\\xa0\\xa0 so everything is in non-right place there is no\\xa0\\none place you can go to to shut down the bitcoin\\xa0\\xa0 server there is no one place you can go to and\\xa0\\nsay i'm gonna arrest that ceo because i'm banning\\xa0\\xa0 this thing and you know we don't want this product\\xa0\\nanymore there's no such because it's decentralized\\xa0\\xa0 it's being run by lots and lots of people all\\xa0\\nacross the world and um so that is the idea of\\xa0\\xa0 what we're 3.0 is that we're going to move away\\xa0\\nfrom from having things being run in a sense in a\\xa0\\xa0 centralized manner to having more votes and having\\xa0\\nmore people involved in the same thing and people\\xa0\\xa0 pushing it and i think it's probably the only\\xa0\\nreason why bitcoin has survived as long as it has\\xa0\\xa0 survived it is resistant to um to being you know\\xa0\\ncontrolled or to being shut down because it is\\xa0\\xa0 be it is run by everybody across the world in all\\xa0\\nthe multiple countries okay so that is the first\\xa0\\xa0 feature of decentralization but there is also\\xa0\\ntrustless and permissions okay so this is like\\xa0\\xa0 um okay well i'll have to go a lot of detail into\\xa0\\nexplaining um you know crypto and decentralization\\xa0\\xa0 to explain what these trustees have permissions\\xa0\\ndo what they're talking about but they are all\\xa0\\xa0 features of this new um you know decentralization\\xa0\\nis that um for the technology to work and to for\\xa0\\xa0 you to be truly decentralized of course you you\\xa0\\nyou need to um you can't have a trust system you\\xa0\\xa0 don't have one central authority that you trust\\xa0\\nokay because the mathematics and the and the and\\xa0\\xa0 the you know the the coding uh manages the you\\xa0\\nknow the logic of how everything works together\\xa0\\xa0 okay so for me to send one bitcoin to another\\xa0\\nperson i need to have bitcoin in my wallet it\\xa0\\xa0 needs to be confirmed it needs to have been mined\\xa0\\nit needs to um you know be available on the ledger\\xa0\\xa0 on the system so i don't need to trust i don't\\xa0\\nneed to go to the to to the to the bank's head\\xa0\\xa0 office and give them my account number for them\\xa0\\nto to prove to you know for them to confirm\\xa0\\xa0 that i've got this amount because it's on the\\xa0\\nsystem it's recorded it's on the ledger okay\\xa0\\xa0 so you don't need to have a central authority that\\xa0\\nhas trust to manage the system because the system\\xa0\\xa0 is managed by author by mathematics the system\\xa0\\nis managed by code the system is managed by the\\xa0\\xa0 um what we call you know the you know the security\\xa0\\nmeasures behind um you know the hashing and the\\xa0\\xa0 you know and the key generation and the fact that\\xa0\\nyou know it's really difficult to crack um you\\xa0\\xa0 know the kind of encryption that goes behind you\\xa0\\nknow um all of that so we don't have to trust a\\xa0\\xa0 central authority we trust the mathematics we\\xa0\\ntrust the coding we trust the key generation\\xa0\\xa0 we trust the fact that you know um the system has\\xa0\\nbeen designed to be temper proof and that is what\\xa0\\xa0 is going to be going towards the future so now we\\xa0\\nno longer need a central authority we no longer\\xa0\\xa0 need a whole um you know bank to manage our money\\xa0\\nwe can manage it with our mathematics and our\\xa0\\xa0 you know so forth okay so that is what um so so\\xa0\\nthen you might think is is web 3.0 completely\\xa0\\xa0 just cryptocurrency no okay that's the beginning\\xa0\\nof it and i think a lot of the web 3.0 um\\xa0\\xa0 discussions and and and even labeling this as the\\xa0\\nnext iteration of the internet started around the\\xa0\\xa0 whole uh concept of cryptocurrency and what\\xa0\\nhappened you know with when bitcoin hit the in\\xa0\\xa0 he the market and so forth but to be honest\\xa0\\ni think the concept has has evolved over time\\xa0\\xa0 that um today it's no longer just about that\\xa0\\nit's about um you know artificial intelligence\\xa0\\xa0 it's actually thinking about the for the in the\\xa0\\nfourth industrial revolution the whole you know\\xa0\\xa0 intelligent way of doing things the internet of\\xa0\\nthings the you know being able to combine like\\xa0\\xa0 look at iota being able to combine the internet\\xa0\\nof things with um with blockchain technology\\xa0\\xa0 being able to combine artificial intelligence with\\xa0\\nblockchain technology we're sort of moving towards\\xa0\\xa0 um you know all of that in combination is is is\\xa0\\nwhat you know is termed a web 3.0 it's definitely\\xa0\\xa0 um it's the next evolution of of the internet it's\\xa0\\nthe next industrial revolution into the future and\\xa0\\xa0 i think if you're a developer and you're new in\\xa0\\nthe industry perhaps it's something that you want\\xa0\\xa0 to start learning and investing a lot of time\\xa0\\nand effort in growing um you know your knowledge\\xa0\\xa0 in that space um and if i must mention something\\xa0\\ni'm a developer and my development journey funny\\xa0\\xa0 enough believe it or not actually started with\\xa0\\nblockchain development okay i started blockchain\\xa0\\xa0 development before i did web development right\\xa0\\nback in the beginning that's me just sharing my\\xa0\\xa0 story i used to do if there i'm talking i see all\\xa0\\nthis all these things that you see today that are\\xa0\\xa0 creating a habit i see them back then before they\\xa0\\nwere even back then in the days of crypto kitties\\xa0\\xa0 okay so before it was like a huge before nfcs\\xa0\\nwere even a huge thing before people were still\\xa0\\xa0 asking what the hell are these and we didn't\\xa0\\ncall them nfts back then we used to call them\\xa0\\xa0 there were there was a name for it you know\\xa0\\nbecause ethereum used to have a token standard\\xa0\\xa0 right so they were the first token standard that\\xa0\\nyou could create a token on top of ethereum and\\xa0\\xa0 i think the idea was that um people wanted to\\xa0\\nto do um icos you know there was the day the\\xa0\\xa0 age of ico's uh independent coin offerings what\\xa0\\nwas it yeah i see also let's say you wanted to\\xa0\\xa0 build your own blockchain right but you need to\\xa0\\nfund it because you need to get money instead of\\xa0\\xa0 going to vcs and getting vcs to find your token\\xa0\\nwhat they used to do back then was build on top\\xa0\\xa0 of ethereum their own token right so i could\\xa0\\nbuild my own token like a scholar token like\\xa0\\xa0 whatever it is i build it and it was like a sort\\xa0\\nof a fundraising method or a sort of an initial\\xa0\\xa0 you know coin offering where i would then um put\\xa0\\nit out in the market and you could you could trade\\xa0\\xa0 these coins even on like binance on big exchanges\\xa0\\nyou know coins that were based off ethereum\\xa0\\xa0 and and people would buy these coins based on\\xa0\\nabsolutely nothing based on like a little white\\xa0\\xa0 paper all you had to do was to drop the white\\xa0\\npaper that said we are gonna we're selling this\\xa0\\xa0 token because we want to build something with it\\xa0\\nright and people would buy these tokens and took\\xa0\\xa0 it to blow up and people were just selling nothing\\xa0\\nthey were selling a and um and and and icos like\\xa0\\xa0 blue out in the water like a big thing but they\\xa0\\nwere they were fungible tokens so they were like\\xa0\\xa0 similar to ethereum if i have one ethereum it's\\xa0\\nthe same as another ethereum okay if i had one\\xa0\\xa0 fungible token it was the same as another fungible\\xa0\\ntoken i could exchange my talking for another one\\xa0\\xa0 i could trade it it could increase in value but\\xa0\\nthere was nothing to differentiate a specific\\xa0\\xa0 token from another one right and that was the path\\xa0\\nand then ethereum came with the next uh standard\\xa0\\xa0 and the next generation and then they created\\xa0\\nthem non-fungible tokens which were um similar\\xa0\\xa0 to the talk to the fungible ones except you could\\xa0\\nidentify a specific token with its identity so i\\xa0\\xa0 would if i had a token um a fungible token i mean\\xa0\\nan unfundable token um you know you could you\\xa0\\xa0 could decide the ownership and you could always\\xa0\\ntell one one bitcoin was not the same as another\\xa0\\xa0 one right so one token was not the same as another\\xa0\\none and you could differentiate its identity its\\xa0\\xa0 ownership its unique signature to to that specific\\xa0\\ntoken and that's when cryptokitties first came out\\xa0\\xa0 and they blew up you know like because you could\\xa0\\nhave a specific cat that had specific features\\xa0\\xa0 specific genetic traits and some features were\\xa0\\nrarer than others others were more common than\\xa0\\xa0 others you had like all sorts of features that you\\xa0\\ncould like you know put together and and who used\\xa0\\xa0 to buy this these cats and i used to have a couple\\xa0\\nof them as well and you would like over some time\\xa0\\xa0 that cats would be able to you know sort of like\\xa0\\nmate with each other you know so you could like\\xa0\\xa0 buy like male female cats and they could mate you\\xa0\\nknow like okay we didn't have male people you had\\xa0\\xa0 like what they call sears and but they were the\\xa0\\nsame same thing same methodology and then the cats\\xa0\\xa0 could like produce markets so you could have like\\xa0\\nso you could have breed these kids and you could\\xa0\\xa0 have like you know maybe you get lucky and and you\\xa0\\nget the genetic lottery and you get a cat that has\\xa0\\xa0 those rare features which were the most wanted\\xa0\\nfeatures and you could sell like a cat with like\\xa0\\xa0 rare features you know um for like a whole lot of\\xa0\\nmoney so imagine horse breeding right you know how\\xa0\\xa0 horse breeding works you know like when you wanna\\xa0\\nlike breed like a prized horse you know you want\\xa0\\xa0 a horse with speed and all of that and sometimes\\xa0\\nyou need to get two really strong horses to breed\\xa0\\xa0 like a winter horse you know something like that\\xa0\\nlike you should think about the in the the mats of\\xa0\\xa0 you know which features do i want in a cat to give\\xa0\\nme the highest chances of me breeding a winner cat\\xa0\\xa0 and but it was all like made up stuff you know at\\xa0\\nthe end of the day but it was fun and interesting\\xa0\\xa0 we used to play these games i need to have\\xa0\\na couple of cats i think if i'm going to my\\xa0\\xa0 um my ethereum wallet i'm still going\\xa0\\nto find i still have some and if\\xa0\\xa0 they're available i don't know maybe\\xa0\\ni'll trade them i probably i still have\\xa0\\xa0 i still have crypto kitties from from those olden\\xa0\\ndays i'm sure i do in my metamask if i can figure\\xa0\\xa0 out my flipping passcode and secret water because\\xa0\\na lot of coils have been lost because people lost\\xa0\\xa0 keys and i'm sure i've lost mine as well and i've\\xa0\\nbeen using it's been like such a long time ago\\xa0\\xa0 but the point i'm trying to make here is that the\\xa0\\nidea of non-fungible tokens came from that right\\xa0\\xa0 and then they and then the whole uh cryptocurrency\\xa0\\nmarket went downhill in 2018 right we had the\\xa0\\xa0 whole bitcoin going from like you know close to\\xa0\\n20k a coin down to 3 000 a coin in like two months\\xa0\\xa0 and and then everything was just like bleak like\\xa0\\ncompanies were closing down i mean they were great\\xa0\\xa0 crypto startups they just went bankrupt the ico\\xa0\\ncraze died down you know everything was like\\xa0\\xa0 you know for like a year and a half you know the\\xa0\\nmarket just had to cool down and i think that's\\xa0\\xa0 around the time that i left my cryptocurrency\\xa0\\nspace as well because at that time i couldn't\\xa0\\xa0 make any money doing anything right um and i don't\\xa0\\nknow why i didn't have the idea back then to do a\\xa0\\xa0 youtube channel on crypto because i should be big\\xa0\\nby now if i had started back then right but then\\xa0\\xa0 i decided to completely change directions and go\\xa0\\ninto web development because um it was easy that\\xa0\\xa0 it was an easier move to go from cryptocurrency\\xa0\\ndevelopment to web developer because now\\xa0\\xa0 suddenly you know i can build stuff people want\\xa0\\ntoday right i can build actual things they can pay\\xa0\\xa0 me for now because at that time i just needed\\xa0\\nthe money but now you know what what people\\xa0\\xa0 is making a comeback it took me a while to catch\\xa0\\nthe wagon so i think in my channel i'm gonna start\\xa0\\xa0 including some of these videos going forward\\xa0\\ni know my blockchain uh content gets a lot of\\xa0\\xa0 attention even though it's like basic stuff you\\xa0\\nknow but maybe what i need to start building on\\xa0\\xa0 is is this kind of and if you and you think this\\xa0\\nis a good journey for us to to go on let me know\\xa0\\xa0 okay so maybe within two three months i will start\\xa0\\ndoing some videos on this but today specifically\\xa0\\xa0 the reason why i created this video was i\\xa0\\nwant to start a journey on machine learning ai\\xa0\\xa0 development and and i'm not teaching you\\xa0\\nbasic um you know um understanding ai\\xa0\\xa0 um you know models and things like that i'm i'm\\xa0\\ntrying to teach you using how to use those tools\\xa0\\xa0 that are available in ai to build real solutions\\xa0\\nfor people today so this is still a development\\xa0\\xa0 um learning a platform we are not a theoretical\\xa0\\nlearning platform so i'm not going to be teaching\\xa0\\xa0 you the theory the theories of artificial\\xa0\\nintelligence or understanding the details of\\xa0\\xa0 how the models work okay i'm not teaching that\\xa0\\nbut i'm going to teach you um having you know\\xa0\\xa0 tools out there how can you use those tools\\xa0\\nto build real solutions that you can actually\\xa0\\xa0 monetize today all right so starting with ai let's\\xa0\\nget into that and and something i've been trying\\xa0\\xa0 to do for like a couple of months now is using the\\xa0\\nopen ai all right so we're gonna go into open ai\\xa0\\xa0 which is over here all right open ai is a\\xa0\\num it's a it's an artificial intelligence\\xa0\\xa0 um i don't know how to put it it's a\\xa0\\nnatural intelligence sort of startup\\xa0\\xa0 and you and i'm gonna link this in your um\\xa0\\ndescription uh below and you can go on it but um i\\xa0\\xa0 think elon musk is one of the people behind it and\\xa0\\na couple of other guys who wanted to create an ai\\xa0\\xa0 api for people to use that is already pre-trained\\xa0\\non certain you know with certain with the data\\xa0\\xa0 first of all that he needs but also with um you\\xa0\\nknow with the models the right model so they've\\xa0\\xa0 done the ai work in the background and they've\\xa0\\ncreated these models that are able to produce you\\xa0\\xa0 know human-like um you know content or human-like\\xa0\\ncommunication like you know if you wanted to\\xa0\\xa0 to like um write um something a human person\\xa0\\na human being would say whether it's an email\\xa0\\xa0 or a a post on facebook um it would be completely\\xa0\\nhuman you know it would look like it was written\\xa0\\xa0 by human being and uh the ai learns from obviously\\xa0\\nmillions and millions of facebook posts about how\\xa0\\xa0 human beings would talk about a specific topic\\xa0\\nright and um and they've already built that so\\xa0\\xa0 you don't have to go and start gathering the data\\xa0\\nand start training models and start figuring out\\xa0\\xa0 you know what is the right model what is\\xa0\\nthe right features what are all the hyper\\xa0\\xa0 parameters i got to use you don't have to all\\xa0\\nof that so you don't have to have to know the ai\\xa0\\xa0 tech lingo or the ai details but you can use the\\xa0\\napi for your own use case of course of course it's\\xa0\\xa0 not free but um it's affordable enough that you\\xa0\\ncan build something with it and monetize it okay\\xa0\\xa0 and i'll give you some examples right there is a\\xa0\\nlot of startups today that use this okay i'll give\\xa0\\xa0 you one one that comes to my paper type right\\xa0\\npapertap.ai is a startup that i discovered on\\xa0\\xa0 producthunt fun enough and it did really really\\xa0\\nwell on product i think it was even product of\\xa0\\xa0 the month at some point let me just read this and\\xa0\\nmake sure yeah it was even number three product\\xa0\\xa0 of the month so he did really really well okay\\xa0\\nso whatever type does is that and i think i've\\xa0\\xa0 talked about this on this on this channel in the\\xa0\\npast is that and there's many like it it's one of\\xa0\\xa0 maybe 50 that i've seen in the last couple of\\xa0\\nmonths so these startups are booming up everywhere\\xa0\\xa0 right and all of them have got the same theory\\xa0\\nin the background okay they're using this type of\\xa0\\xa0 tool methodologies maybe they build their own ai i\\xa0\\ndon't know i don't know how the backend works but\\xa0\\xa0 it's a similar type of ai that is available\\xa0\\non open ai all right where um you know these\\xa0\\xa0 ais have been trained with um human-like data\\xa0\\nmillions and millions of human-like data to be\\xa0\\xa0 able to produce human-like content so let's say\\xa0\\nyou are a marketing person and you want to create\\xa0\\xa0 a facebook post and you have to create like 10\\xa0\\n20 facebook posts a day and you just don't have\\xa0\\xa0 the intellectual capacity or the strength\\xa0\\nto think up 10 20 different unique ideas\\xa0\\xa0 in like a space of eight hours right um you could\\xa0\\nprobably do it but like sometimes these things\\xa0\\xa0 are just they just take a lot of effort to just\\xa0\\nthink right and you can put then ideas into an ai\\xa0\\xa0 and a i can generate for you random content\\xa0\\nthat is human readable that is actually looks\\xa0\\xa0 like it was written by human being okay for this\\xa0\\ncontent of yours and then you can like you know\\xa0\\xa0 you can do that so if you look at paper type\\xa0\\nthey've got like you know you can like you can\\xa0\\xa0 look at products right um where is it products\\xa0\\nuh try paper type ai maybe they want to ask me\\xa0\\xa0 to log in i don't know but i want to show you\\xa0\\nthe kind of things it does you can produce\\xa0\\xa0 blog content okay so i'm gonna leave that one\\xa0\\nand and and and and but there's another one let\\xa0\\xa0 me give you another one okay writer like i said i\\xa0\\nknow a lot of these a lot of these um you know ai\\xa0\\xa0 stuff right so if you look at writer for example\\xa0\\nright and you can pick like like whatever you want\\xa0\\xa0 to write about blog idea description primary\\xa0\\nkeyword i want to write about um you know\\xa0\\xa0 what is it what is it what is an interesting\\xa0\\nstop topic learn learn e-commerce e-commerce web development right and produce one a variant\\xa0\\nand write for me right this is how simple this\\xa0\\xa0 kind of air things work right and you give it\\xa0\\nan idea you give it exactly what you want it to\\xa0\\xa0 to produce for you and it goes and looks into\\xa0\\num a lot of the you know the training that is\\xa0\\xa0 done in the background with information and\\xa0\\nthat it has and it it creates for you content\\xa0\\xa0 that you can work with so so this so this is\\xa0\\nlike i'm trying to find blog ideas and outline\\xa0\\xa0 so this is a typical blog you can write about\\xa0\\ne-commerce the essential guide to e-commerce web\\xa0\\xa0 development for beginners that are the right\\xa0\\neven gives you the keywords that it has used so\\xa0\\xa0 that when you later on want to get more details\\xa0\\nyou can use those keywords right how to build\\xa0\\xa0 your first online store what are the popular\\xa0\\npayment gateways for e-commerce the amazing\\xa0\\xa0 benefits of building e-commerce sites using\\xa0\\nwordpress look at that these are all ideas it came\\xa0\\xa0 up with based on its own training okay and writer\\xa0\\nis really really good so how does writer work\\xa0\\xa0 in the background it's got maybe not exactly\\xa0\\nthe same because i don't really know the stack\\xa0\\xa0 behind a writer i'm just guessing but it's got\\xa0\\nsomething similar to what's available on open ai\\xa0\\xa0 all right um it's got these engines that you can\\xa0\\nuse to get specific stuff and train it and you\\xa0\\xa0 know um you know that it gives you the content and\\xa0\\nif you can learn open ai maybe maybe you can build\\xa0\\xa0 a web application like this for yourself and you\\xa0\\ncan charge it because this this this ai writers\\xa0\\xa0 are going for i mean if you look at javas which\\xa0\\nis the most popular one right if you know javas.ai\\xa0\\xa0 java charges what up to 90 a month okay\\xa0\\nthis is where i started okay i should pay 90\\xa0\\xa0 a month for this ai writers because they\\xa0\\nreally honestly save you a lot of time\\xa0\\xa0 and that's [ __ ] ridiculous to pay this amount\\xa0\\nof money for that all right but this is one of the\\xa0\\xa0 oldest the biggest well-known and i'm sure java's\\xa0\\nprobably wrote their own um you know code behind\\xa0\\xa0 i don't know how their background how what stack\\xa0\\nthey're using in the back if they're using open ai\\xa0\\xa0 but i can tell you that the majority of these\\xa0\\ntools that exist today in the in the industry\\xa0\\xa0 they use open ai um api and and if you think about\\xa0\\nit if you can charge even 29 which is what writer\\xa0\\xa0 charges a month for clients you know you only need\\xa0\\nto make like what 10 20 100 clients to make a good\\xa0\\xa0 you know amount of money from this and trust\\xa0\\nme there is always a market for this this is an\\xa0\\xa0 industry that is just hitting a place where it's\\xa0\\ngonna boom up and this is a brilliant startup idea\\xa0\\xa0 um of something you can build uh all right so i'm\\xa0\\ngonna take you today through um going through this\\xa0\\xa0 open ai all right so we're gonna go and create an\\xa0\\naccount because because i tried in like before in\\xa0\\xa0 the there was a whole long you know process to get\\xa0\\nuh you know approvals and it was like trying to\\xa0\\xa0 get approvals for like you know using the you know\\xa0\\nthe facebook api it was like a nightmare forever\\xa0\\xa0 but um now apparently um the approvals are not\\xa0\\nso you know tedious and you can like almost get\\xa0\\xa0 started with your api keys and it's easier to sort\\xa0\\nof get you use the api um you know than it was in\\xa0\\xa0 the past because obviously they don't want you\\xa0\\nto abuse this thing they don't want you to build\\xa0\\xa0 something that for example is not honest about the\\xa0\\nfact that the content is ai generated they don't\\xa0\\xa0 want you to build something that uh maybe is gonna\\xa0\\njust start producing random content um and and\\xa0\\xa0 and and and not think through because there's a\\xa0\\ndanger of obviously with this kind of technologies\\xa0\\xa0 and i think they had to be careful about it then\\xa0\\nit makes sense why they had to do it because\\xa0\\xa0 if you're gonna start creating random blogs that\\xa0\\nare not even like thought through by human beings\\xa0\\xa0 and they're all ai generated how long until the\\xa0\\nwhole internet is just full of um you know because\\xa0\\xa0 it's gonna just produce the same stuff um i mean\\xa0\\nit's gonna just re you know how ai works it uses\\xa0\\xa0 history to to predict and make something of the\\xa0\\nfuture but now if all of your stuff is gonna be ai\\xa0\\xa0 then the history is just going to repeat yourself\\xa0\\nyou know you'll never break out of there'll never\\xa0\\xa0 be innovation they'll never be like human thinking\\xa0\\nso there's a danger to having too many of these\\xa0\\xa0 tools around people trying to control um the\\xa0\\nreach but also there's a danger to um you know\\xa0\\xa0 if companies just all they do is generate ai\\xa0\\ncontent instead of actually generating real\\xa0\\xa0 uh human uh you know so so i think that's what\\xa0\\nthey're trying to avoid but anyway let's go\\xa0\\xa0 ahead and and do this let's go ahead and create an\\xa0\\naccount i don't know if i should do it via google\\xa0\\xa0 or let me let me let me let me do it via google i\\xa0\\nthink it'll be faster and let me do it via google\\xa0\\xa0 let's do let's use our scholar online google\\xa0\\naccount and um log in and see if we can create a\\xa0\\xa0 um you know an account using that okay all\\xa0\\nright i am scholar online yes verify my phone\\xa0\\xa0 number so let me do that i'm gonna enter my phone\\xa0\\nnumber here if i have if i can find my phone okay i'm exploring for personal use awesome\\xa0\\nthat was amazing okay let's get started\\xa0\\xa0 with the open api now when you first create an\\xa0\\naccount this is a page that you're gonna land\\xa0\\xa0 on okay um i'm really so excited that they've\\xa0\\ndecided to remove the whole verification\\xa0\\xa0 process that you know i could never get past you\\xa0\\nknow before because i wasn't building anything\\xa0\\xa0 serious and i didn't have the time and the\\xa0\\neffort to go through their verification but now\\xa0\\xa0 it looks like you can definitely test things\\xa0\\nout for free not really free because they do\\xa0\\xa0 give you a bit of credit which is going to run\\xa0\\nout immediately and i'll explain to you how all\\xa0\\xa0 of that works okay so the way api the way that um\\xa0\\nso let me call this the open ai sort of engines\\xa0\\xa0 or models all right they call them\\xa0\\nengines but they're really ai models right\\xa0\\xa0 so there are different types of models that are\\xa0\\navailable on the on the open air and they all have\\xa0\\xa0 different strengths and weaknesses and different\\xa0\\ncapabilities depending on what you're trying to\\xa0\\xa0 get at okay so this is the basic foundation of\\xa0\\nbeing able to understand how to work with open\\xa0\\xa0 ai okay so the very first and the most commonly\\xa0\\nused model is the da vinci model okay but it is\\xa0\\xa0 also the most expensive model and if you look\\xa0\\nat all their examples when you run the examples\\xa0\\xa0 on the website which you will go through later\\xa0\\nthe default model is the da vinci one which is\\xa0\\xa0 the most expensive one okay however it is the\\xa0\\nmost capable it is the most advanced it has all\\xa0\\xa0 the things that you believe the majority of the\\xa0\\nthings that you probably want to do with open ai\\xa0\\xa0 and um it is also um the slowest okay because it\\xa0\\nhas obviously more complicated model it costs more\\xa0\\xa0 it is also the slowest okay and so you need to\\xa0\\nsort of weigh you know how what what accuracy do\\xa0\\xa0 you want out of your engine versus how much money\\xa0\\nyou're willing to spend on it and if you're gonna\\xa0\\xa0 monetize this how much are your clients willing\\xa0\\nto pay to get this um you know results out\\xa0\\xa0 the next one which is just as capable if not\\xa0\\nmore is the curio engine okay the query engine\\xa0\\xa0 is a little bit faster and it cost a much much\\xa0\\nless all right so if you look at this list as\\xa0\\xa0 you go from top to bottom the models are going to\\xa0\\nbecome a little bit faster but maybe a little bit\\xa0\\xa0 less capability and less complex okay so that\\xa0\\ndavinci is the most capable is the most complex\\xa0\\xa0 is the most expensive and is the slowest okay and\\xa0\\nthen you can go to the cute one it's just a little\\xa0\\xa0 bit beneath that it is um capable enough but\\xa0\\nit's gonna be way faster and definitely cheaper\\xa0\\xa0 because it uses less resources and then after\\xa0\\nthat you get to the babbage one which is even\\xa0\\xa0 faster however it is um you know less capable than\\xa0\\nthe first two models as well but it also depends\\xa0\\xa0 on what you're trying to achieve okay so these\\xa0\\nmodels are not all created equally and when you\\xa0\\xa0 have time do go through the documentation\\xa0\\nbecause there is a lot of explanations\\xa0\\xa0 and definitions out of the documentation i'm a\\xa0\\nbeliever of um i know it's easy to watch youtube\\xa0\\xa0 videos but if you're going to build something\\xa0\\nlike this and you're going to be serious about it\\xa0\\xa0 invest in the time to go and read this\\xa0\\ndocumentation in a lot of detail so that\\xa0\\xa0 you can understand what is available and what\\xa0\\ncan be done and how much it costs and what\\xa0\\xa0 are the different models because if you look\\xa0\\nover here i've tried to summarize this okay\\xa0\\xa0 the curie for example is good for conversational\\xa0\\nand if you want to do language translations\\xa0\\xa0 but babbage you can use beverage because it's\\xa0\\ngonna be cheaper and if you're just doing like\\xa0\\xa0 searching and classifications you can use that\\xa0\\nfor sure so it depends on what you're trying\\xa0\\xa0 to achieve okay so text classification like\\xa0\\nfor example you want to classify semantics or\\xa0\\xa0 tweets for example you wanna maybe you've got\\xa0\\nlike a bunch of tweets and um you know you're\\xa0\\xa0 building an air model to classify you know the\\xa0\\nthe sort of the tone or the you know what is the\\xa0\\xa0 tweet all about you can use aid ada which is the\\xa0\\ncheapest the fastest right but if you want to do\\xa0\\xa0 like pro that generate ideas and and content\\xa0\\nfor example blog generation perhaps that's a\\xa0\\xa0 little bit more complicated and maybe ada won't be\\xa0\\nable to do a good job on it then we're gonna try\\xa0\\xa0 and work with something like davinci or curio\\xa0\\nso it depends on you're trying to do okay and\\xa0\\xa0 like i said take the time to read through the\\xa0\\ndocumentation because depending on what the use\\xa0\\xa0 case is and what you're trying to achieve you're\\xa0\\ngonna probably work with the different angels okay\\xa0\\xa0 then the next thing i like about open ai as well\\xa0\\nis that there's good examples all right so there's\\xa0\\xa0 decent examples on the website that you can work\\xa0\\nwith that i'm gonna work with one of the examples\\xa0\\xa0 just to test that everything works okay i\\xa0\\ncreated this account right here in front of you\\xa0\\xa0 i haven't paid anything for it okay i'm just gonna\\xa0\\ntry and test some of the things out to figure out\\xa0\\xa0 you know really how much is how does the um you\\xa0\\nknow the you know the free trial work and you\\xa0\\xa0 know with these credits how do they also work as\\xa0\\nwell okay so there's a page also that explains the\\xa0\\xa0 credits because you're gonna need to understand\\xa0\\num how much tokens you're using because every\\xa0\\xa0 time you make an api call you don't get charged\\xa0\\nthey don't charge you per dollar or pay you know\\xa0\\xa0 prefix financial costs they charge you per\\xa0\\ntoken okay and the token um you know um sort of\\xa0\\xa0 represent a certain output okay so if you're gonna\\xa0\\nuh be looking for like a log a long explanation or\\xa0\\xa0 a long product description which has lots of words\\xa0\\nand you want two or three different versions of it\\xa0\\xa0 okay you're going to be using up more tokens and\\xa0\\nthen it's going to cost you more but if you're\\xa0\\xa0 doing something simple like you just want maybe\\xa0\\none sentence of summarization which uses less\\xa0\\xa0 tokens it's going to cost you less so they charge\\xa0\\nyou based on the output that you're getting out of\\xa0\\xa0 it and um depending on how you so if you're going\\xa0\\nto monetize this think about that as well and how\\xa0\\xa0 you are going to charge your clients for the use\\xa0\\ncases that you're going to be providing of this\\xa0\\xa0 open ai okay the next thing that i want to cover\\xa0\\nis some other things randomness temperature but\\xa0\\xa0 we'll get into that a little bit more detail when\\xa0\\nwe're looking at our example but what i want to\\xa0\\xa0 discuss um now is the free trial okay so when you\\xa0\\nget started now i really like what they've done\\xa0\\xa0 is that you can now get 18 of a free trial\\xa0\\nokay so that means that you can actually test\\xa0\\xa0 out this api you can use the different engines\\xa0\\nand you can do a lot of stuff and um you can do\\xa0\\xa0 it for free um until you use up your 18 and then\\xa0\\nafter that you're gonna have to start paying and\\xa0\\xa0 you're gonna have to enter a credit card which\\xa0\\nwill be built as you use um the opinion so it's\\xa0\\xa0 definitely not free and if you're going to use\\xa0\\nthis to create a use case or you know monetizes\\xa0\\xa0 you are gonna have to charge your clients for this\\xa0\\nand it's not going to be available for free but\\xa0\\xa0 in my opinion whatever they're charging you for it\\xa0\\nis way cheaper than you trying to build your own\\xa0\\xa0 um artificial intelligence engine and even getting\\xa0\\nthe data for it we just i mean i'm on the what\\xa0\\xa0 these guys have done for me is really amazing to\\xa0\\nmake ai more accessible to the average developer\\xa0\\xa0 and definitely if your clients are going to use\\xa0\\nthis i don't expect to expect to use it for free\\xa0\\xa0 so you can you know you can you can you can\\xa0\\ntake the cost down to your client and build them\\xa0\\xa0 interesting solutions and they will uh pay for it\\xa0\\nokay so this is how it works okay so davinci like\\xa0\\xa0 i said is the most expensive one so for 18 18\\xa0\\nwill get you 300 tokens on da vinci right but\\xa0\\xa0 the same 18 will get you 3 million tokens on\\xa0\\nkiri so you can see the difference in price\\xa0\\xa0 between damaging curry it's orders of magnitude\\xa0\\nmuch much more expensive okay so i would use\\xa0\\xa0 davinci if i really have to use it it is the most\\xa0\\nexpensive one okay so 3 million tokens on curie\\xa0\\xa0 for 18 but the same 18 will get you 15 million on\\xa0\\nbabbage and 22.5 millions on ada okay so as you go\\xa0\\xa0 down the models uh they become cheaper so you\\xa0\\ncan get more out of the same back by using a\\xa0\\xa0 cheaper models okay but depends again like i\\xa0\\nsaid on the use case itself all right so um with\\xa0\\xa0 that in mind and we've got 30 15 18 3 18 let's\\xa0\\nput it to the test and try out some stuff okay\\xa0\\xa0 so um the next a page is a page full of examples\\xa0\\nokay so you've got idea generations and there's a\\xa0\\xa0 lot of these ai tools and i told you specifically\\xa0\\nthat i'm going to be trying to rebuild some of\\xa0\\xa0 these tools that i use that are that charge you\\xa0\\nlike what twenty nine dollars a month or whatever\\xa0\\xa0 um if you think about it blog topic is one of\\xa0\\nthe most uh i think most commonly used use cases\\xa0\\xa0 out there with this ai writers okay if you can\\xa0\\ngenerate block topics from an idea okay um how\\xa0\\xa0 would you could use um one of these engines for it\\xa0\\nokay and um you only all you have to do is to give\\xa0\\xa0 it a heading all right like like the example they\\xa0\\ngive you here is block topics dealing with a daily\\xa0\\xa0 life living on mars okay so you give you that idea\\xa0\\nblock topics within that field and then you let it\\xa0\\xa0 generate for you ideas on the different types of\\xa0\\nof of blog topics you can do let's see try another\\xa0\\xa0 okay the machine atmosphere whatever it is okay so\\xa0\\nyou can think about the different types of um blog\\xa0\\xa0 topics you want to ask and then you can get ideas\\xa0\\nand remember you can do a lot of this you know\\xa0\\xa0 you can you can tag lines youtube video titles so\\xa0\\nyou can you can even say create titles for youtube\\xa0\\xa0 videos and then you give you some ideas of the\\xa0\\ntitles it can create and then it completes all of\\xa0\\xa0 that for you so you can go wild and i think this\\xa0\\nis where you can even use things like you know\\xa0\\xa0 facebook or posts twitter tweets whatever you\\xa0\\nknow so this is how they actually build those\\xa0\\xa0 kind of things so let's say you wanted let's work\\xa0\\nwith the blog topics cause i think it's the most\\xa0\\xa0 obvious one let's say you wanted to create\\xa0\\na blog topic when you're going through this\\xa0\\xa0 documentation and i will link this you'll see at\\xa0\\nthe top they they've got documentation they've got\\xa0\\xa0 examples and they have a playground and then\\xa0\\nwithin the documentations you can go through\\xa0\\xa0 um the documentations all the way there you've got\\xa0\\nthe api reference okay you've got the guides which\\xa0\\xa0 give you some examples and some you know things\\xa0\\nyou can use to function your your your your models\\xa0\\xa0 and then obviously they get started the quick\\xa0\\nstart so with python obviously you you definitely\\xa0\\xa0 want to start here on the on the on the developer\\xa0\\nquick start because you want to figure out\\xa0\\xa0 you know your api keys and and how to at least set\\xa0\\nthat up in python right so your api key is already\\xa0\\xa0 generated when you get started so what you need to\\xa0\\ndo is just to copy this api key all right so we're\\xa0\\xa0 going to copy this api key and then we're going to\\xa0\\ngo to where we're running our model because i've\\xa0\\xa0 already created a directory over here and as you\\xa0\\ncan see i already have my um virtual environment\\xa0\\xa0 activated within where i'm going to be working\\xa0\\nfrom so all i want you to do is either save the\\xa0\\xa0 api key inside of your of your environment or in\\xa0\\nmy specific case like how i like to do it i create\\xa0\\xa0 a file called config.py and save it in there all\\xa0\\nright so copy that key create a file what however\\xa0\\xa0 where you you save your your api keys whether\\xa0\\nyou're gonna do it in an environment or whatever\\xa0\\xa0 do that so that you can access it instead of your\\xa0\\ncode all right once you've done that we're gonna\\xa0\\xa0 come back and um there's some examples on how to\\xa0\\nmake a request um then what you're going to use\\xa0\\xa0 um so when you create a request okay you start\\xa0\\nwith um so this is like a typical um you know um\\xa0\\xa0 you know curl request all right but um i i like\\xa0\\nworking with python okay so the i think they've\\xa0\\xa0 got python examples as well all right you can see\\xa0\\nthere's pathetic this is the pattern examples at\\xa0\\xa0 the bottom here so the first thing you need to\\xa0\\ndo is to pip install open ai so just copy that\\xa0\\xa0 and make sure that your virtual environment is\\xa0\\nactivated and paste pip install open ai which\\xa0\\xa0 i've already done okay so you do that and then run\\xa0\\nthat program okay so once you've done that it's\\xa0\\xa0 going to go through and it's going to install open\\xa0\\nai for you and then inside of a file typically you\\xa0\\xa0 will have um you know um you will initiate your\\xa0\\napi key by saying open a i don't api key and then\\xa0\\xa0 you're gonna put the api key there all right\\xa0\\nwhile you're testing you can even hard code it\\xa0\\xa0 i don't mind um and then figure out how you know\\xa0\\nlater on when you go into production to save it\\xa0\\xa0 your environment or save it in a different file\\xa0\\nand then i'm creating a response is as simple as\\xa0\\xa0 just calling the open ai completion create okay\\xa0\\nso when you create you first of all you need to\\xa0\\xa0 specify your engine okay what engine are you gonna\\xa0\\nworking with these are the engines that i started\\xa0\\xa0 defining at the beginning where you've got your da\\xa0\\nvinci curie babbage ada and remember they've got\\xa0\\xa0 different capabilities and a different prices okay\\xa0\\nso depending on the engine that you're gonna use\\xa0\\xa0 um you're gonna pay differently and different\\xa0\\ncampaigns and you can see they're giving you 18\\xa0\\xa0 or free but remember 18 only get you 300 tokens\\xa0\\non da vinci and 3 million on curry so maybe\\xa0\\xa0 you want to at least start with curie which is\\xa0\\nmore capable than all the other ones but a little\\xa0\\xa0 bit cheaper you get three million tokens out of it\\xa0\\nokay and then you can um say you know what it is\\xa0\\xa0 that you want to um the prompt because this is a\\xa0\\ncompletion so it's gonna complete for you so this\\xa0\\xa0 prompt is where you would put in what you want\\xa0\\nthe api to return so in this specific case let's\\xa0\\xa0 say you wanted to return blog ideas if you if you\\xa0\\ngo back to our example of blog ideas where is it i think it's over here um examples um um is it\\xa0\\nidea generation let me open it in a new tab so\\xa0\\xa0 if you go back to our idea generation examples um\\xa0\\nit's taking a bit of time where is it there you go\\xa0\\xa0 and we want to do blog topics you can if\\xa0\\nyou open if you look at i'm opening this\\xa0\\xa0 in playground let's open this in playground for\\xa0\\nexample and um in playground you can actually\\xa0\\xa0 like test this thing out playground is like\\xa0\\nsort of like think of it as an online build\\xa0\\xa0 um you know what you call it um a place where you\\xa0\\ncan where you can like you know run your code and\\xa0\\xa0 a lot of apis have got this and you can test these\\xa0\\nthings out you know think of a a postman but it's\\xa0\\xa0 like already pre-built into the into the browser\\xa0\\nand it's pre-built into this website and already\\xa0\\xa0 the you know the api calls are sort of built\\xa0\\nin the back end um you can view the code you\\xa0\\xa0 can like rank them in the playground it's gonna\\xa0\\nuse your api keys by the way and it's going to\\xa0\\xa0 use your credits that you have available because\\xa0\\nyou need to be logged in here to be able to use\\xa0\\xa0 this playground so it's got everything run for\\xa0\\nyou and you can use this for testing whatever\\xa0\\xa0 you can test all the different things that are\\xa0\\navailable so over here you've got your temperature\\xa0\\xa0 your response length there's a lot of features\\xa0\\nthese are these features that you gotta uh edit\\xa0\\xa0 here uh the same as um where will be before let's\\xa0\\ngo back here you know you've got these features\\xa0\\xa0 that you can add um on the api call on the create\\xa0\\napi call okay so they're not all included they max\\xa0\\xa0 tokens but when you're working on the playground\\xa0\\nyou'll see that there's a lot that you can adjust\\xa0\\xa0 and read the documentation and if you hover\\xa0\\non top of it as well it will explain to you\\xa0\\xa0 what this means and how you can edit it you know\\xa0\\ncontrols diversity so you can go from zero to one\\xa0\\xa0 for example if you understand what is temperature\\xa0\\nyou can hover over it and it will explain to you\\xa0\\xa0 okay temperature controls the randomness okay\\xa0\\nso you can start with zero up to one okay one is\\xa0\\xa0 less random and zero is a more no no one is more\\xa0\\nrandom and zero is less random this means that\\xa0\\xa0 um if you wanna get more variety of answers um you\\xa0\\nwanna have a high temperature as possible okay so\\xa0\\xa0 you want to go a little bit higher you'll get\\xa0\\nbut then also with more randomness you might\\xa0\\xa0 not get responses and also there is a case where\\xa0\\nyou can like not get anything at all so sometimes\\xa0\\xa0 you want to play with this to get the right you\\xa0\\nknow for your thing and maybe you just want to\\xa0\\xa0 like you know it depends on what you're asking it\\xa0\\nto do okay and over time as you test this thing\\xa0\\xa0 out you'll figure out where is a good place to\\xa0\\nhave your temperatures depending on the use case\\xa0\\xa0 that you're giving it okay and then here\\xa0\\nyou can decide the response length obviously\\xa0\\xa0 you can go all the way from one and this is your\\xa0\\ntokens okay you can go all the way up to 2000\\xa0\\xa0 and obviously the more your response length um the\\xa0\\nmore expensive and the more you're gonna pay so i\\xa0\\xa0 would like leave it the way it was at 64. where\\xa0\\nwas it i would leave it where it was at 64. i\\xa0\\xa0 actually would leave all the defaults where they\\xa0\\nwere and see what i generate first and then start\\xa0\\xa0 tweaking them to test their sensitivity to the to\\xa0\\nset to to test their results sensitivity to the\\xa0\\xa0 whatever then you'll figure out how all of this\\xa0\\nworks okay so i think that 18 will be sufficient\\xa0\\xa0 for you to play around with this and figure out\\xa0\\nwhat you get out okay and the frequency penalty\\xa0\\xa0 so there's a lot of things that you can play\\xa0\\naround here but um what's cool about this as\\xa0\\xa0 well is at the top there you can press that view\\xa0\\ncode button and you can see an example of a code\\xa0\\xa0 um that that's running this so you can run\\xa0\\nit yourself in python okay so what i did i\\xa0\\xa0 just copied all of this and i pasted it inside of\\xa0\\npython so just copy all of that like plus the copy\\xa0\\xa0 button all right and then that's it and then we're\\xa0\\ngonna go into our python and we're gonna say nano\\xa0\\xa0 um you know i'm gonna open a python document and\\xa0\\ni'm gonna call it a blogs a blog dot y i think yes\\xa0\\xa0 okay so yeah that's the one right and just paste\\xa0\\neverything in here okay the slight differences\\xa0\\xa0 because i'm not saving my um token in the\\xa0\\nenvironment i've created a file called config\\xa0\\xa0 and i've saved my token in there and the way that\\xa0\\ni access it is to say is open ai dot api key and\\xa0\\xa0 then you say config.open ai api key this is what\\xa0\\ni call the variable inside of that file called\\xa0\\xa0 config and then i'm going to access it like that\\xa0\\nand then but everything else is exactly the same\\xa0\\xa0 response open air completion create all of that\\xa0\\nblock topics on dealing with daily life on mars\\xa0\\xa0 i'm going to use the exact same example and then\\xa0\\ni've added just a line at the bottom there that\\xa0\\xa0 prints the response so i can see the kind of\\xa0\\nresponse i'm getting back from the open ai\\xa0\\xa0 and what i'm going to do here is i'm going to\\xa0\\nchange my engine from um let's see i'm going\\xa0\\xa0 to change it from davinci which is expensive i'm\\xa0\\ngoing to change it to query all right so you can\\xa0\\xa0 do that over there and you can change the engine\\xa0\\nor this is like your model okay this is the model\\xa0\\xa0 in the background so i'm going to change this to\\xa0\\num q re all right and then i'm going to save this\\xa0\\xa0 and then i'm going to run this file and then we're\\xa0\\ngoing to see what we get out all right so let's\\xa0\\xa0 say python blog dot api and let's run that and it\\xa0\\ntakes a while to run but i think it will be faster\\xa0\\xa0 than if we ran that and you'll see it didn't\\xa0\\nrespond anything okay so um a finished reason stop\\xa0\\xa0 index is zero um null text it didn't uh\\xa0\\nprovide any responses so i didn't get anything\\xa0\\xa0 back all right so i'm going to go back to opening\\xa0\\num uh blog.py and let's see changing this back to\\xa0\\xa0 um the engine back to um to davinci alright\\xa0\\nso let's change it back to davinci let's just\\xa0\\xa0 copy that and i'll paste that in there all right\\xa0\\nobviously i want to probably make this a small d\\xa0\\xa0 all right and then let's run this and run python\\xa0\\nblock.py and and this time it takes again a little\\xa0\\xa0 bit longer but it it does give you some responses\\xa0\\nyou can see at the top there the text was nothing\\xa0\\xa0 it gave 10 but it didn't give me anything in\\xa0\\nresult in response all right but now it has\\xa0\\xa0 given me some results okay so what you can do you\\xa0\\ncan play around with the temperatures over there\\xa0\\xa0 so if you open um nano blocked or py you can play\\xa0\\naround with at the temperature with the max tokens\\xa0\\xa0 this top p1 frequency penalty and and and\\xa0\\nwhatever and and so you can see you know what\\xa0\\xa0 generates more but i think your biggest thing\\xa0\\nto work with also is the type of model that it\\xa0\\xa0 that you wanted to create so davinci is good at\\xa0\\ncreating content remember what i had in here that\\xa0\\xa0 curry is powerful at language translation complex\\xa0\\nclassification text sentiment summarization\\xa0\\xa0 so it might not be good at creating you know uh um\\xa0\\nyou know um it takes maybe that's why nothing was\\xa0\\xa0 created even though you can get more tokens out\\xa0\\nof it it doesn't mean that all of these models\\xa0\\xa0 can do everything that you want them to do okay\\xa0\\nso if i had something that i was trying to text\\xa0\\xa0 a classified text for example like i had a tweet\\xa0\\nthat i'm trying to classify perhaps that's what\\xa0\\xa0 i would use qre4 but if i i'm trying to create\\xa0\\nperhaps davinci is the only one you can use for\\xa0\\xa0 that but then keep in mind the tokens and how\\xa0\\nthey get used up right so the last thing that i\\xa0\\xa0 want to cover here today is you can keep track of\\xa0\\nyour account over here you know so account usage\\xa0\\xa0 if you go to account usage and you refresh that i\\xa0\\nknow i've used some of the tokens let's rerun this\\xa0\\xa0 and see where i am in terms of my um spending so\\xa0\\nyou want to check this on a regular basis you see\\xa0\\xa0 i've got some um i've used some of the tokens\\xa0\\nthere okay so i've used like a cent out of that 18\\xa0\\xa0 all right just to generate this so you can see\\xa0\\nthat actually you know you can and there's the\\xa0\\xa0 13 prompt 56 completion 69 tokens less than\\xa0\\na dollar 77 tokens 36 tokens that have used\\xa0\\xa0 and i have about 300 000 tokens so that's actually\\xa0\\nquite a bit okay so if you're paying if you're\\xa0\\xa0 gonna pay like a hundred dollars i mean you can do\\xa0\\na lot with this and you can definitely create one\\xa0\\xa0 of those things that you charge people um you know\\xa0\\nthis kind of if you think about how much jarvis is\\xa0\\xa0 charging for their thing at 98 a month a per user\\xa0\\nyou know and you get 100 users a thousand users\\xa0\\xa0 and trust me not all those users are going to be\\xa0\\nutilizing all their tokens by the end of the month\\xa0\\xa0 and a hundred dollars of tokens per month that's\\xa0\\na lot per person okay so i think if you really\\xa0\\xa0 think about a a cool use case you can uh get a\\xa0\\nreally like cash cow with this um open ai okay\\xa0\\xa0 and even the open ki team um haven't even\\xa0\\ncompletely completely um described all the\\xa0\\xa0 use cases that you can get up to there i've\\xa0\\nactually still waiting for guys to come up\\xa0\\xa0 with ideas because they're giving you the tools\\xa0\\nthey're giving you the engines it's up to you to\\xa0\\xa0 come up with a creativity and say if i had an ai\\xa0\\nthat can generate text what would i want to build\\xa0\\xa0 that is really useful for our people that would\\xa0\\nwould pay me for it okay for me so for me the\\xa0\\xa0 first thing that comes to mind that i really like\\xa0\\nto build for example is um a tweet generator all\\xa0\\xa0 right being able to generate a random tweet right\\xa0\\ni'd like to be able to do that based on the topic\\xa0\\xa0 right and then you use that to build some\\xa0\\nautomation software with um with i know i know\\xa0\\xa0 probably they won't allow you to build they want\\xa0\\nto approve that use case because i already have\\xa0\\xa0 seen that being in the to no no list of the open\\xa0\\nai being able to generate a random tweet generator\\xa0\\xa0 okay that you would then build another uh python\\xa0\\nbot that would then randomly treat those things\\xa0\\xa0 for you so you could keep your twitter running\\xa0\\nwithout actually having to think anything and\\xa0\\xa0 you can just like tweet three four times a day\\xa0\\nrandomly from the ai generator and you got the api\\xa0\\xa0 a twitter that just posts that for you and then\\xa0\\nyou look like you're really busy on twitter when\\xa0\\xa0 you're not so i'd like to build that but i think\\xa0\\nyou can do it with open air they want to approve\\xa0\\xa0 it i've seen it on the no no list but maybe\\xa0\\nyou know you can i'll try some other uh you\\xa0\\xa0 know ai tools in the future to see how i can do\\xa0\\nthat because that's what i'd love to build okay\\xa0\\xa0 so thank you guys for watching what we're gonna\\xa0\\ndo next week is that we're gonna expand a little\\xa0\\xa0 bit on this and we're gonna actually build our own\\xa0\\nwe're going to start working on building our own\\xa0\\xa0 ai like our own jarvis okay so we're going to\\xa0\\nwork on building our own jackets let's just\\xa0\\xa0 make it clear we're going to work on building our\\xa0\\nown javas uh platform where um it will be doing\\xa0\\xa0 random ai you know not random but it will be\\xa0\\ngenerating ai content for users and um we people\\xa0\\xa0 people can you know from the front end of the app\\xa0\\nenter what they want so something like writer for\\xa0\\xa0 example if you if you think of writer you know\\xa0\\nthis and you can like you know start writing and\\xa0\\xa0 uh build this kind of platform um for yourself and\\xa0\\ncharge people to use it what up to what how much\\xa0\\xa0 is java's charge like in how many lake and you\\xa0\\ncan make a lot of money with this because i can\\xa0\\xa0 tell you there is a need for this in the market\\xa0\\nthis is like the next big thing if you look at\\xa0\\xa0 then the new apps that are being developed this is\\xa0\\nthe future of um of of you know web 3.0 definitely\\xa0\\xa0 one of the things you can do in the web 3.0\\xa0\\nspace okay so thank you guys for watching i've\\xa0\\xa0 spent this week thinking about a nice app that\\xa0\\nwe're gonna build using um this open ai platform\\xa0\\xa0 and um if you're interested leave me a comment\\xa0\\nbelow if you like this video or you just want\\xa0\\xa0 to go back to jungle development let me know\\xa0\\neither way and we'll see you guys next week\", lookup_str='', metadata={'source': 'mBVaf3FnVL8&list=PLATQCFQn9lGe3xJmyb6ZRENcHptqNR5gs', 'title': 'Into in to Web3 and OpenAI API Programming with Python', 'description': '#python #openai #web3\\nInto in to Web3 and OpenAI API Programming with Python\\n😀 video stamps 👇🏾👇🏾 Use to Jump ahead the video 😀\\n\\nBuy me a Beer\\nhttps://www.buymeacoffee.com/feyitaje\\n\\nIntroduction in to Web3, what is Web3\\nhttps://www.investopedia.com/web-20-web-30-5208698\\n\\nGPT3 Wikipedia\\nhttps://en.wikipedia.org/wiki/GPT-3\\n\\nOPEN-AI API create an account\\nhttps://openai.com/blog/openai-api/\\n\\n---------------------------------------------------------------------------------------------------------------\\nVideo Timestamps:\\n00:00:33 What is web 3.0\\n00:16:20 What is OpenAI? Explore their API\\n00:20:04 Exploring AI content writers that exist, a possible use case for OpenAI\\n00:22:28 Create your own OpenAI account to get started\\n00:26:02 Explore the different engine or models that you can use with OpenAI - from davinci, curie, babbage, ada\\n00:31:01 OpenAI free trial, what you can get for free\\n00:34:00 Generate blog topic ideas code example with OpenAI Davinci engine - python code\\n\\n---------------------------------------------------------------------------------------------------------------\\n\\nSocial Media Links\\nFollow our Google News Publication:\\nhttps://news.google.com/publications/CAAqBwgKMNGEngsw8o62Aw/sections/CAQqEAgAKgcICjDRhJ4LMPKOtgMw6oLqBg?oc=3&ceid=ZA:en \\n\\nMedium Publication: https://skolo-online.medium.com/\\n\\nFollow us on Facebook: https://www.facebook.com/Skolo-Online-Learning-101030254983171\\n\\nEmail me: admin@skolo.online\\n\\nFollow us on Twitter: https://twitter.com/skolo_online\\n\\nFollow us on Instagram: https://www.instagram.com/skolo_online/\\n\\nLearn more on Skolo Online\\nhttps://skolo.online\\n\\n---------------------------------------------------------------------------------------------------------\\n\\nGet Professionals to Build your Web Application\\n\\nhttps://tati.digital \\n\\n---------------------------------------------------------------------------------------------------------\\nVideo Tags\\n\\n#ai #artificialintelligence #openaiapi', 'view_count': 10664, 'thumbnail_url': 'https://i.ytimg.com/vi/mBVaf3FnVL8/sddefault.jpg', 'publish_date': datetime.datetime(2021, 12, 4, 0, 0), 'length': 2973, 'author': 'Skolo Online'}, lookup_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Video 2"
      ],
      "metadata": {
        "id": "WlmgYUwZVRO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = YoutubeLoader.from_youtube_channel('https://www.youtube.com/watch?v=v6sF8Ed3nTE',add_video_info=True)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "v_XjFXAaMWlL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN4vOYG7N7BH",
        "outputId": "1df1a1d6-dedc-4c2f-a16c-43dfe9233361"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=\"[00:00:00] Okay. So a lot of people have been asking me how\\ncould we hook up alpaca to LangChain and try it out for a chat bot kind of thing. You can do this quite easy. I'm gonna show you a way in this video, and\\nthen later on in another video I'll show you some ways that we could take what we fine\\ntuned our special version and use that as well. So you're going to need to install Transformers\\nfrom the main branch, from GitHub. If you just install the normal pip version\\nyou won't get the version with the right tokenizer and the right model for doing this. So you can basically just bring that in. You're also gonna need bits and bytes. If you're gonna do an eight bit version and\\nyou of course, you're gonna need lang chain. so what we're gonna be doing is we're gonna\\nbe using the hugging face LLM wrapper in LangChain to run this so you can see that. Okay. First off, we are just gonna be bringing in\\nour llama, tokenizer and Lama, token and lama for causal language model. remember that alpaca is built on llama and\\nis made for a model that's the [00:01:00] similar style that as our alpaca is. So that's why we are using theseFrom LangChain. We're gonna be bringing in the Hugging Face\\nPipeline . We're also gonna need to just bring in, some other things as we go. So first off, I'm bringing in the template\\nand the LM chain, just to show you the, okay. We can bring this in. We can set up, our model. We've bringing it in as eight bits so that\\nwe're gonna use less memory and it'll be faster for inference. . And here is where a lot of the magic happens\\nis that the way that the Lama model and alpaca model are set up we can actually set up a\\nhugging face pipeline for them, which is gonna be doing text generation. And we just then just pass in this model into\\nhere. We pass in the tokenizer, we pass in things\\nlike our max length, our temperature our, top p and the repetition penalty in here as\\nwell. and then we can set up a local. LLM from this hugging face pipeline. So this is actually, coming from LangChain\\nand it's allowing us to set [00:02:00] up a LLM with this hugging face pipeline there. Once we've got that done, we can basically\\ntry it out with just a normal LLM chain. So I'm just gonna take one of the standard\\nalpaca prompts. And style of prompts for the template. So we create a prompt template in here. We pass this in and then you can see we're\\nbasically just setting up our prompt template with the template that we've made here. And then the instruction is just gonna be\\nthe variable that we're gonna pass in. So this is standard stuff. We've then basically got this going on here\\nso we can have, what is the capital of England? And sure enough this is just going to inject\\nthis question into where the instruction goes in here. And then it's gonna give us our answer out. Our answer out is gonna be the capital of\\nEngland is London. And then if I ask it, the typical sort of\\nalpaca question of, okay, what are alpacas and how are they different from llamas? You can see here we get, the standard answer\\nthat we got when we just [00:03:00] played around with, the model by itself without LangChain. All right, so setting up the conversation\\npart is one of the key things is we want to make use of the memory here. So we are going to Lama and alpaca have a\\ngood sized token span. So we can actually go up to sort of 2000 tokens\\nhere. What we're doing now, unfortunately, we fine\\ntuned it probably for a lot less than that. So it will be interesting to see. And I would like to hear back from you guys\\nas well. How well does it do when you get a really\\nlong token span in there? . It may do really good. You might find it at times that it doesn't\\ndo as good but okay. So we're gonna set up our conversation chain. So if you remember, one of the things that\\nwe used in a conversation chain is the whole idea of a memory. And the particular memory that I've chosen\\nto use here is this conversation, buffer window memory. So what this is gonna do is give us a window\\nthat we pass across the conversation. [00:04:00] and we are going to represent K\\nnumber of turns. . So for example, in this case, I've decided\\nto set K to four, which means we're gonna have four turns going on. And that will be the limit. So this should keep us so our token span never\\ngets too wide in the conversation. All right, setting up the conversation chain. We just passed in our local LLM that we set\\nup earlier with the hugging face pipeline. We're gonna pass in our memory to be the,\\nthis window memory that we've created. And we're gonna just set verbose equals true\\nso we can just see what's going on. If we look at the the template here, So just\\nto see what the template, the standard template is it looks something like this. The following is a friendly conversation between\\na human and an ai. The AI is talkative and provides lots of specific\\ndetails from its context. If the AI does not know the answer, it truthfully\\nsays it does not know. And so you can see in this prompt we're injecting\\ntwo things. We're injecting the human input and we're\\nalso injecting the [00:05:00] history or the memory. That's coming of the conversation as it's\\ngone through. So I decided to modify this a little bit. So here's the version that I modified it to. And you obviously play around with this yourself. The following is a friendly conversation between\\na human and an AI called alpaca. So we wanted to have a little bit of knowledge\\nabout who it is. So we could also put some things in there\\nthat, Alpaca is three years old. Alpaca loves to eat apples. You could put some other things in there too. I find it's always fun. To watch people play with this if you've set\\nit up to be quite funny as well, so you could try that out. so to do that, we basically just override\\nthe conversation dot prompt dot template. So you can see I'm just taking that new conversation\\nprompt template and trying it out. . so now it's got our name being alpaca and\\nstuff in there. All right, now we get to talking to it. the first thing that I tried, Al, was basically\\njust saying, hi, there I am Sam. . So it's interesting here. we know that the alpaca is [00:06:00] fine\\ntuned, not on dialogue, right? We haven't done a version that's fine tuned\\non dialogue yet. Maybe that will be a future video. But here we've done it's fine tuned on tasks,\\nso they tend to be tasks where you ask it. It to give you a fact, or you ask it to reproduce\\na list of something. That kind of thing. So when I don't actually ask it something,\\nit doesn't do a great job. It's perhaps a little bit confused in here. And what does it do? It doesn't generate bad text. It just then goes on to generate more text\\nthan we asked for. So it's taken in, you know that oh, it's talking\\nto Sam. Hey there, Sam, it's nice to meet you. What can I help you with? And then it generates Sam's response back\\nof, do you know what time it is, and then alpaca. Sure. Sure do. It's currently, and then it obviously passing\\nin a token for where you could substitute the time. You could do a regex change or something there\\nand put in the current time. . So then I realized that, okay, this is probably\\nhappening because we [00:07:00] didn't ask it a question. So it's trying to just keep being chatty and\\nmake up things itself. So if we try it and we ask it, okay, what\\nis your name? So now it's much more on point, right? It's okay, my name is Alpaca. How may I help you today? . So we've now got multiple turns in here,\\nbut we probably shouldn't count the turn where it generated. It's Sam and it's an alpaca. It's really should be. Human AI is one turn in there. now, I ask it another question. Can you tell me what an alpaca is? Sure. It's gives us an answer. And it's not too long here, you can see that\\nan alpaca is a species of a South American camelid mammal. They're typically brown or white in color\\nand have long necks and legs, All right. So then I ask how is it different than a lama? So you see at this point now we've got these\\nmultiple steps of our memory being passed in, of the conversation. So we've got all the steps from the start\\nbeing passed in here cuz we haven't met the sort of length of the window where it needs\\nto [00:08:00] start cutting things off yet. Okay. How is it different than a llama? Alpacas and llamas are both members of the\\ncamelaide family? I'm not sure how you pronounce that. But they differ in several ways. Alpacas tend to be smaller than llamas and\\nthey're, and it then it stops. so it should have been able to go on not ideal\\nagain, remember, this hasn't been fine tuned for conversation. So we are just using the fine tuned version\\nof Alpaca. can you gimme some good names for a pet Lama? Now if we go back to here, look at the memory. So this is the next thing I'm gonna ask it. You can see we've lost the start of the conversation. So where I said, hi, my name is Sam. You know that's all been taken out now. We're going as far back now as where I asked,\\nwhat is your name? So we've got one human, two human, three human,\\nfour human and then this being passed in. so our memory is a window of four that we've\\ngot going on there. Now [00:09:00] you could experiment with a\\nlonger memory if you want to. But remember all this is going Into the language\\nmodel. So you will find that if your memory gets\\ntoo long, then it can actually be, quite slow to do things. Alright. So I ask it, can you give me some good names\\nfor a pet lama? Sure. Here are some great options. Pata, Tika, cushy and Wayra. Okay. And then I wanted to test it, see, does it\\nstill remember its name? So I ask you, is your name Fred? And you can see that we've lost The bit in\\nthe conversation where it told us our name. So it's only relying on the name being up\\nthere. Yet still it understands from the context. No. My name is Alpaca. So that's good. We next asked it. Okay, what food should I feed my new llama? So again, you see where our window of four\\nis staying fixed, so we are losing things that we spoke about early on in the conversation. Okay. And then finally, your new llama can eat grass,\\nhay even alfalfa. You can also try giving them some [00:10:00]\\nvegetables like carrots, apples, and banana. . So this is just setting it up now. You could actually try and mess with the prompt\\nmore to basically inject the whole personality in there. Do some things like that. But the idea here is that it turns out that\\nllama's doing pretty well. I wouldn't say great, but it's doing pretty\\nwell with the prompt for the chatbot and then passing in this memory that we've got going\\non the current conversation. And then generating out. So this is something you could try an experiment\\nwith and this would also work on the llama model if you want to actually try the llama\\nmodel. My guess is that probably won't do as well\\nas the alpaca model. I don't know, maybe I will try that out and\\nwe can look at that in another video. Anyway, as always, if you have any questions\\nplease put them in the. And if this was useful to you please click\\nand subscribe. I will see you in the next video. Bye for now.\", lookup_str='', metadata={'source': 'v6sF8Ed3nTE', 'title': 'Talking to Alpaca with LangChain - Creating an Alpaca Chatbot', 'description': 'Colab notebook: https://drp.li/XapBR\\n\\nIn this video lets have a play with talking to an Alpaca7B model using LangChain with a conversational chain and a memory window.', 'view_count': 8577, 'thumbnail_url': 'https://i.ytimg.com/vi/v6sF8Ed3nTE/sddefault.jpg', 'publish_date': datetime.datetime(2023, 3, 22, 0, 0), 'length': 658, 'author': 'Sam Witteveen'}, lookup_index=0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Bs_XIGOLK2",
        "outputId": "bd480bc8-e639-4696-ceb0-3d64e4162ab5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00:00] Okay. So a lot of people have been asking me how\n",
            "could we hook up alpaca to LangChain and try it out for a chat bot kind of thing. You can do this quite easy. I'm gonna show you a way in this video, and\n",
            "then later on in another video I'll show you some ways that we could take what we fine\n",
            "tuned our special version and use that as well. So you're going to need to install Transformers\n",
            "from the main branch, from GitHub. If you just install the normal pip version\n",
            "you won't get the version with the right tokenizer and the right model for doing this. So you can basically just bring that in. You're also gonna need bits and bytes. If you're gonna do an eight bit version and\n",
            "you of course, you're gonna need lang chain. so what we're gonna be doing is we're gonna\n",
            "be using the hugging face LLM wrapper in LangChain to run this so you can see that. Okay. First off, we are just gonna be bringing in\n",
            "our llama, tokenizer and Lama, token and lama for causal language model. remember that alpaca is built on llama and\n",
            "is made for a model that's the [00:01:00] similar style that as our alpaca is. So that's why we are using theseFrom LangChain. We're gonna be bringing in the Hugging Face\n",
            "Pipeline . We're also gonna need to just bring in, some other things as we go. So first off, I'm bringing in the template\n",
            "and the LM chain, just to show you the, okay. We can bring this in. We can set up, our model. We've bringing it in as eight bits so that\n",
            "we're gonna use less memory and it'll be faster for inference. . And here is where a lot of the magic happens\n",
            "is that the way that the Lama model and alpaca model are set up we can actually set up a\n",
            "hugging face pipeline for them, which is gonna be doing text generation. And we just then just pass in this model into\n",
            "here. We pass in the tokenizer, we pass in things\n",
            "like our max length, our temperature our, top p and the repetition penalty in here as\n",
            "well. and then we can set up a local. LLM from this hugging face pipeline. So this is actually, coming from LangChain\n",
            "and it's allowing us to set [00:02:00] up a LLM with this hugging face pipeline there. Once we've got that done, we can basically\n",
            "try it out with just a normal LLM chain. So I'm just gonna take one of the standard\n",
            "alpaca prompts. And style of prompts for the template. So we create a prompt template in here. We pass this in and then you can see we're\n",
            "basically just setting up our prompt template with the template that we've made here. And then the instruction is just gonna be\n",
            "the variable that we're gonna pass in. So this is standard stuff. We've then basically got this going on here\n",
            "so we can have, what is the capital of England? And sure enough this is just going to inject\n",
            "this question into where the instruction goes in here. And then it's gonna give us our answer out. Our answer out is gonna be the capital of\n",
            "England is London. And then if I ask it, the typical sort of\n",
            "alpaca question of, okay, what are alpacas and how are they different from llamas? You can see here we get, the standard answer\n",
            "that we got when we just [00:03:00] played around with, the model by itself without LangChain. All right, so setting up the conversation\n",
            "part is one of the key things is we want to make use of the memory here. So we are going to Lama and alpaca have a\n",
            "good sized token span. So we can actually go up to sort of 2000 tokens\n",
            "here. What we're doing now, unfortunately, we fine\n",
            "tuned it probably for a lot less than that. So it will be interesting to see. And I would like to hear back from you guys\n",
            "as well. How well does it do when you get a really\n",
            "long token span in there? . It may do really good. You might find it at times that it doesn't\n",
            "do as good but okay. So we're gonna set up our conversation chain. So if you remember, one of the things that\n",
            "we used in a conversation chain is the whole idea of a memory. And the particular memory that I've chosen\n",
            "to use here is this conversation, buffer window memory. So what this is gonna do is give us a window\n",
            "that we pass across the conversation. [00:04:00] and we are going to represent K\n",
            "number of turns. . So for example, in this case, I've decided\n",
            "to set K to four, which means we're gonna have four turns going on. And that will be the limit. So this should keep us so our token span never\n",
            "gets too wide in the conversation. All right, setting up the conversation chain. We just passed in our local LLM that we set\n",
            "up earlier with the hugging face pipeline. We're gonna pass in our memory to be the,\n",
            "this window memory that we've created. And we're gonna just set verbose equals true\n",
            "so we can just see what's going on. If we look at the the template here, So just\n",
            "to see what the template, the standard template is it looks something like this. The following is a friendly conversation between\n",
            "a human and an ai. The AI is talkative and provides lots of specific\n",
            "details from its context. If the AI does not know the answer, it truthfully\n",
            "says it does not know. And so you can see in this prompt we're injecting\n",
            "two things. We're injecting the human input and we're\n",
            "also injecting the [00:05:00] history or the memory. That's coming of the conversation as it's\n",
            "gone through. So I decided to modify this a little bit. So here's the version that I modified it to. And you obviously play around with this yourself. The following is a friendly conversation between\n",
            "a human and an AI called alpaca. So we wanted to have a little bit of knowledge\n",
            "about who it is. So we could also put some things in there\n",
            "that, Alpaca is three years old. Alpaca loves to eat apples. You could put some other things in there too. I find it's always fun. To watch people play with this if you've set\n",
            "it up to be quite funny as well, so you could try that out. so to do that, we basically just override\n",
            "the conversation dot prompt dot template. So you can see I'm just taking that new conversation\n",
            "prompt template and trying it out. . so now it's got our name being alpaca and\n",
            "stuff in there. All right, now we get to talking to it. the first thing that I tried, Al, was basically\n",
            "just saying, hi, there I am Sam. . So it's interesting here. we know that the alpaca is [00:06:00] fine\n",
            "tuned, not on dialogue, right? We haven't done a version that's fine tuned\n",
            "on dialogue yet. Maybe that will be a future video. But here we've done it's fine tuned on tasks,\n",
            "so they tend to be tasks where you ask it. It to give you a fact, or you ask it to reproduce\n",
            "a list of something. That kind of thing. So when I don't actually ask it something,\n",
            "it doesn't do a great job. It's perhaps a little bit confused in here. And what does it do? It doesn't generate bad text. It just then goes on to generate more text\n",
            "than we asked for. So it's taken in, you know that oh, it's talking\n",
            "to Sam. Hey there, Sam, it's nice to meet you. What can I help you with? And then it generates Sam's response back\n",
            "of, do you know what time it is, and then alpaca. Sure. Sure do. It's currently, and then it obviously passing\n",
            "in a token for where you could substitute the time. You could do a regex change or something there\n",
            "and put in the current time. . So then I realized that, okay, this is probably\n",
            "happening because we [00:07:00] didn't ask it a question. So it's trying to just keep being chatty and\n",
            "make up things itself. So if we try it and we ask it, okay, what\n",
            "is your name? So now it's much more on point, right? It's okay, my name is Alpaca. How may I help you today? . So we've now got multiple turns in here,\n",
            "but we probably shouldn't count the turn where it generated. It's Sam and it's an alpaca. It's really should be. Human AI is one turn in there. now, I ask it another question. Can you tell me what an alpaca is? Sure. It's gives us an answer. And it's not too long here, you can see that\n",
            "an alpaca is a species of a South American camelid mammal. They're typically brown or white in color\n",
            "and have long necks and legs, All right. So then I ask how is it different than a lama? So you see at this point now we've got these\n",
            "multiple steps of our memory being passed in, of the conversation. So we've got all the steps from the start\n",
            "being passed in here cuz we haven't met the sort of length of the window where it needs\n",
            "to [00:08:00] start cutting things off yet. Okay. How is it different than a llama? Alpacas and llamas are both members of the\n",
            "camelaide family? I'm not sure how you pronounce that. But they differ in several ways. Alpacas tend to be smaller than llamas and\n",
            "they're, and it then it stops. so it should have been able to go on not ideal\n",
            "again, remember, this hasn't been fine tuned for conversation. So we are just using the fine tuned version\n",
            "of Alpaca. can you gimme some good names for a pet Lama? Now if we go back to here, look at the memory. So this is the next thing I'm gonna ask it. You can see we've lost the start of the conversation. So where I said, hi, my name is Sam. You know that's all been taken out now. We're going as far back now as where I asked,\n",
            "what is your name? So we've got one human, two human, three human,\n",
            "four human and then this being passed in. so our memory is a window of four that we've\n",
            "got going on there. Now [00:09:00] you could experiment with a\n",
            "longer memory if you want to. But remember all this is going Into the language\n",
            "model. So you will find that if your memory gets\n",
            "too long, then it can actually be, quite slow to do things. Alright. So I ask it, can you give me some good names\n",
            "for a pet lama? Sure. Here are some great options. Pata, Tika, cushy and Wayra. Okay. And then I wanted to test it, see, does it\n",
            "still remember its name? So I ask you, is your name Fred? And you can see that we've lost The bit in\n",
            "the conversation where it told us our name. So it's only relying on the name being up\n",
            "there. Yet still it understands from the context. No. My name is Alpaca. So that's good. We next asked it. Okay, what food should I feed my new llama? So again, you see where our window of four\n",
            "is staying fixed, so we are losing things that we spoke about early on in the conversation. Okay. And then finally, your new llama can eat grass,\n",
            "hay even alfalfa. You can also try giving them some [00:10:00]\n",
            "vegetables like carrots, apples, and banana. . So this is just setting it up now. You could actually try and mess with the prompt\n",
            "more to basically inject the whole personality in there. Do some things like that. But the idea here is that it turns out that\n",
            "llama's doing pretty well. I wouldn't say great, but it's doing pretty\n",
            "well with the prompt for the chatbot and then passing in this memory that we've got going\n",
            "on the current conversation. And then generating out. So this is something you could try an experiment\n",
            "with and this would also work on the llama model if you want to actually try the llama\n",
            "model. My guess is that probably won't do as well\n",
            "as the alpaca model. I don't know, maybe I will try that out and\n",
            "we can look at that in another video. Anyway, as always, if you have any questions\n",
            "please put them in the. And if this was useful to you please click\n",
            "and subscribe. I will see you in the next video. Bye for now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92DZvJQFOQtP",
        "outputId": "e9d0e603-8ab0-4e05-958b-36711fed0aa3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': 'v6sF8Ed3nTE', 'title': 'Talking to Alpaca with LangChain - Creating an Alpaca Chatbot', 'description': 'Colab notebook: https://drp.li/XapBR\\n\\nIn this video lets have a play with talking to an Alpaca7B model using LangChain with a conversational chain and a memory window.', 'view_count': 8577, 'thumbnail_url': 'https://i.ytimg.com/vi/v6sF8Ed3nTE/sddefault.jpg', 'publish_date': datetime.datetime(2023, 3, 22, 0, 0), 'length': 658, 'author': 'Sam Witteveen'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from time import time,sleep\n",
        "openai.api_key = \"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\"\n",
        "#\n",
        "def predict_function(prompt):\n",
        "    #\n",
        "    messages = [{\"role\":\"system\",\n",
        "             \"content\":\"Your are a helpful assistant.\",\n",
        "             },\n",
        "            ]\n",
        "    #\n",
        "    messages.append({\"role\":\"user\",\"content\":prompt})\n",
        "    print(messages)\n",
        "    #\n",
        "    max_retry = 5\n",
        "    retry = 0\n",
        "    #\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            chat = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "                                        messages = messages,\n",
        "                                        temperature=0,\n",
        "                                        )\n",
        "            reply = chat.choices[0].message.content \n",
        "            return reply\n",
        "        except Exception as oops:\n",
        "            retry += 1\n",
        "            if retry >= max_retry:\n",
        "                return \"Accessing the Completion service error\"\n",
        "            print('Error communicating with Completion service:', oops)\n",
        "            sleep(1)\n"
      ],
      "metadata": {
        "id": "vYO6gq7dOwrF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"What does the author intend to explain based on the video transcript below:\n",
        "TRANSCRIPT :\n",
        "<<content>>\"\"\""
      ],
      "metadata": {
        "id": "kr4XySG-P1w2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt.replace(\"<<content>>\",documents[0].page_content)"
      ],
      "metadata": {
        "id": "rci1hvs3QHOM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_function(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "zurqHlemQSHy",
        "outputId": "a0d5c442-6203-4370-93f6-1b5c67aa5352"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'Your are a helpful assistant.'}, {'role': 'user', 'content': \"What does the author intend to explain based on the video transcript below:\\nTRANSCRIPT :\\n[00:00:00] Okay. So a lot of people have been asking me how\\ncould we hook up alpaca to LangChain and try it out for a chat bot kind of thing. You can do this quite easy. I'm gonna show you a way in this video, and\\nthen later on in another video I'll show you some ways that we could take what we fine\\ntuned our special version and use that as well. So you're going to need to install Transformers\\nfrom the main branch, from GitHub. If you just install the normal pip version\\nyou won't get the version with the right tokenizer and the right model for doing this. So you can basically just bring that in. You're also gonna need bits and bytes. If you're gonna do an eight bit version and\\nyou of course, you're gonna need lang chain. so what we're gonna be doing is we're gonna\\nbe using the hugging face LLM wrapper in LangChain to run this so you can see that. Okay. First off, we are just gonna be bringing in\\nour llama, tokenizer and Lama, token and lama for causal language model. remember that alpaca is built on llama and\\nis made for a model that's the [00:01:00] similar style that as our alpaca is. So that's why we are using theseFrom LangChain. We're gonna be bringing in the Hugging Face\\nPipeline . We're also gonna need to just bring in, some other things as we go. So first off, I'm bringing in the template\\nand the LM chain, just to show you the, okay. We can bring this in. We can set up, our model. We've bringing it in as eight bits so that\\nwe're gonna use less memory and it'll be faster for inference. . And here is where a lot of the magic happens\\nis that the way that the Lama model and alpaca model are set up we can actually set up a\\nhugging face pipeline for them, which is gonna be doing text generation. And we just then just pass in this model into\\nhere. We pass in the tokenizer, we pass in things\\nlike our max length, our temperature our, top p and the repetition penalty in here as\\nwell. and then we can set up a local. LLM from this hugging face pipeline. So this is actually, coming from LangChain\\nand it's allowing us to set [00:02:00] up a LLM with this hugging face pipeline there. Once we've got that done, we can basically\\ntry it out with just a normal LLM chain. So I'm just gonna take one of the standard\\nalpaca prompts. And style of prompts for the template. So we create a prompt template in here. We pass this in and then you can see we're\\nbasically just setting up our prompt template with the template that we've made here. And then the instruction is just gonna be\\nthe variable that we're gonna pass in. So this is standard stuff. We've then basically got this going on here\\nso we can have, what is the capital of England? And sure enough this is just going to inject\\nthis question into where the instruction goes in here. And then it's gonna give us our answer out. Our answer out is gonna be the capital of\\nEngland is London. And then if I ask it, the typical sort of\\nalpaca question of, okay, what are alpacas and how are they different from llamas? You can see here we get, the standard answer\\nthat we got when we just [00:03:00] played around with, the model by itself without LangChain. All right, so setting up the conversation\\npart is one of the key things is we want to make use of the memory here. So we are going to Lama and alpaca have a\\ngood sized token span. So we can actually go up to sort of 2000 tokens\\nhere. What we're doing now, unfortunately, we fine\\ntuned it probably for a lot less than that. So it will be interesting to see. And I would like to hear back from you guys\\nas well. How well does it do when you get a really\\nlong token span in there? . It may do really good. You might find it at times that it doesn't\\ndo as good but okay. So we're gonna set up our conversation chain. So if you remember, one of the things that\\nwe used in a conversation chain is the whole idea of a memory. And the particular memory that I've chosen\\nto use here is this conversation, buffer window memory. So what this is gonna do is give us a window\\nthat we pass across the conversation. [00:04:00] and we are going to represent K\\nnumber of turns. . So for example, in this case, I've decided\\nto set K to four, which means we're gonna have four turns going on. And that will be the limit. So this should keep us so our token span never\\ngets too wide in the conversation. All right, setting up the conversation chain. We just passed in our local LLM that we set\\nup earlier with the hugging face pipeline. We're gonna pass in our memory to be the,\\nthis window memory that we've created. And we're gonna just set verbose equals true\\nso we can just see what's going on. If we look at the the template here, So just\\nto see what the template, the standard template is it looks something like this. The following is a friendly conversation between\\na human and an ai. The AI is talkative and provides lots of specific\\ndetails from its context. If the AI does not know the answer, it truthfully\\nsays it does not know. And so you can see in this prompt we're injecting\\ntwo things. We're injecting the human input and we're\\nalso injecting the [00:05:00] history or the memory. That's coming of the conversation as it's\\ngone through. So I decided to modify this a little bit. So here's the version that I modified it to. And you obviously play around with this yourself. The following is a friendly conversation between\\na human and an AI called alpaca. So we wanted to have a little bit of knowledge\\nabout who it is. So we could also put some things in there\\nthat, Alpaca is three years old. Alpaca loves to eat apples. You could put some other things in there too. I find it's always fun. To watch people play with this if you've set\\nit up to be quite funny as well, so you could try that out. so to do that, we basically just override\\nthe conversation dot prompt dot template. So you can see I'm just taking that new conversation\\nprompt template and trying it out. . so now it's got our name being alpaca and\\nstuff in there. All right, now we get to talking to it. the first thing that I tried, Al, was basically\\njust saying, hi, there I am Sam. . So it's interesting here. we know that the alpaca is [00:06:00] fine\\ntuned, not on dialogue, right? We haven't done a version that's fine tuned\\non dialogue yet. Maybe that will be a future video. But here we've done it's fine tuned on tasks,\\nso they tend to be tasks where you ask it. It to give you a fact, or you ask it to reproduce\\na list of something. That kind of thing. So when I don't actually ask it something,\\nit doesn't do a great job. It's perhaps a little bit confused in here. And what does it do? It doesn't generate bad text. It just then goes on to generate more text\\nthan we asked for. So it's taken in, you know that oh, it's talking\\nto Sam. Hey there, Sam, it's nice to meet you. What can I help you with? And then it generates Sam's response back\\nof, do you know what time it is, and then alpaca. Sure. Sure do. It's currently, and then it obviously passing\\nin a token for where you could substitute the time. You could do a regex change or something there\\nand put in the current time. . So then I realized that, okay, this is probably\\nhappening because we [00:07:00] didn't ask it a question. So it's trying to just keep being chatty and\\nmake up things itself. So if we try it and we ask it, okay, what\\nis your name? So now it's much more on point, right? It's okay, my name is Alpaca. How may I help you today? . So we've now got multiple turns in here,\\nbut we probably shouldn't count the turn where it generated. It's Sam and it's an alpaca. It's really should be. Human AI is one turn in there. now, I ask it another question. Can you tell me what an alpaca is? Sure. It's gives us an answer. And it's not too long here, you can see that\\nan alpaca is a species of a South American camelid mammal. They're typically brown or white in color\\nand have long necks and legs, All right. So then I ask how is it different than a lama? So you see at this point now we've got these\\nmultiple steps of our memory being passed in, of the conversation. So we've got all the steps from the start\\nbeing passed in here cuz we haven't met the sort of length of the window where it needs\\nto [00:08:00] start cutting things off yet. Okay. How is it different than a llama? Alpacas and llamas are both members of the\\ncamelaide family? I'm not sure how you pronounce that. But they differ in several ways. Alpacas tend to be smaller than llamas and\\nthey're, and it then it stops. so it should have been able to go on not ideal\\nagain, remember, this hasn't been fine tuned for conversation. So we are just using the fine tuned version\\nof Alpaca. can you gimme some good names for a pet Lama? Now if we go back to here, look at the memory. So this is the next thing I'm gonna ask it. You can see we've lost the start of the conversation. So where I said, hi, my name is Sam. You know that's all been taken out now. We're going as far back now as where I asked,\\nwhat is your name? So we've got one human, two human, three human,\\nfour human and then this being passed in. so our memory is a window of four that we've\\ngot going on there. Now [00:09:00] you could experiment with a\\nlonger memory if you want to. But remember all this is going Into the language\\nmodel. So you will find that if your memory gets\\ntoo long, then it can actually be, quite slow to do things. Alright. So I ask it, can you give me some good names\\nfor a pet lama? Sure. Here are some great options. Pata, Tika, cushy and Wayra. Okay. And then I wanted to test it, see, does it\\nstill remember its name? So I ask you, is your name Fred? And you can see that we've lost The bit in\\nthe conversation where it told us our name. So it's only relying on the name being up\\nthere. Yet still it understands from the context. No. My name is Alpaca. So that's good. We next asked it. Okay, what food should I feed my new llama? So again, you see where our window of four\\nis staying fixed, so we are losing things that we spoke about early on in the conversation. Okay. And then finally, your new llama can eat grass,\\nhay even alfalfa. You can also try giving them some [00:10:00]\\nvegetables like carrots, apples, and banana. . So this is just setting it up now. You could actually try and mess with the prompt\\nmore to basically inject the whole personality in there. Do some things like that. But the idea here is that it turns out that\\nllama's doing pretty well. I wouldn't say great, but it's doing pretty\\nwell with the prompt for the chatbot and then passing in this memory that we've got going\\non the current conversation. And then generating out. So this is something you could try an experiment\\nwith and this would also work on the llama model if you want to actually try the llama\\nmodel. My guess is that probably won't do as well\\nas the alpaca model. I don't know, maybe I will try that out and\\nwe can look at that in another video. Anyway, as always, if you have any questions\\nplease put them in the. And if this was useful to you please click\\nand subscribe. I will see you in the next video. Bye for now.\"}]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The author intends to explain how to hook up Alpaca to LangChain for a chatbot using the Hugging Face LLM wrapper in LangChain to run it. The author shows how to set up the model, the tokenizer, and the pipeline for text generation. The author also demonstrates how to set up a conversation chain and use memory to keep track of the conversation history. The author concludes that Alpaca is doing pretty well with the chatbot prompt and passing in the current conversation memory. The author suggests experimenting with the prompt to inject personality and trying the same approach with the Llama model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Youtub Video Summarizer"
      ],
      "metadata": {
        "id": "QbVChf4ZQn6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Generate clear and concise summary based on the video transcript below:\n",
        "TRANSCRIPT :\n",
        "<<content>>\"\"\""
      ],
      "metadata": {
        "id": "RiSWtJwWQdUc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt.replace(\"<<content>>\",documents[0].page_content)"
      ],
      "metadata": {
        "id": "yhpCZKDLQm_R"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_function(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "iIFygxtiQtRl",
        "outputId": "7ba8b530-5af2-43c2-8c26-d7eae189b029"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'Your are a helpful assistant.'}, {'role': 'user', 'content': \"Generate clear and concise summary based on the video transcript below:\\nTRANSCRIPT :\\n[00:00:00] Okay. So a lot of people have been asking me how\\ncould we hook up alpaca to LangChain and try it out for a chat bot kind of thing. You can do this quite easy. I'm gonna show you a way in this video, and\\nthen later on in another video I'll show you some ways that we could take what we fine\\ntuned our special version and use that as well. So you're going to need to install Transformers\\nfrom the main branch, from GitHub. If you just install the normal pip version\\nyou won't get the version with the right tokenizer and the right model for doing this. So you can basically just bring that in. You're also gonna need bits and bytes. If you're gonna do an eight bit version and\\nyou of course, you're gonna need lang chain. so what we're gonna be doing is we're gonna\\nbe using the hugging face LLM wrapper in LangChain to run this so you can see that. Okay. First off, we are just gonna be bringing in\\nour llama, tokenizer and Lama, token and lama for causal language model. remember that alpaca is built on llama and\\nis made for a model that's the [00:01:00] similar style that as our alpaca is. So that's why we are using theseFrom LangChain. We're gonna be bringing in the Hugging Face\\nPipeline . We're also gonna need to just bring in, some other things as we go. So first off, I'm bringing in the template\\nand the LM chain, just to show you the, okay. We can bring this in. We can set up, our model. We've bringing it in as eight bits so that\\nwe're gonna use less memory and it'll be faster for inference. . And here is where a lot of the magic happens\\nis that the way that the Lama model and alpaca model are set up we can actually set up a\\nhugging face pipeline for them, which is gonna be doing text generation. And we just then just pass in this model into\\nhere. We pass in the tokenizer, we pass in things\\nlike our max length, our temperature our, top p and the repetition penalty in here as\\nwell. and then we can set up a local. LLM from this hugging face pipeline. So this is actually, coming from LangChain\\nand it's allowing us to set [00:02:00] up a LLM with this hugging face pipeline there. Once we've got that done, we can basically\\ntry it out with just a normal LLM chain. So I'm just gonna take one of the standard\\nalpaca prompts. And style of prompts for the template. So we create a prompt template in here. We pass this in and then you can see we're\\nbasically just setting up our prompt template with the template that we've made here. And then the instruction is just gonna be\\nthe variable that we're gonna pass in. So this is standard stuff. We've then basically got this going on here\\nso we can have, what is the capital of England? And sure enough this is just going to inject\\nthis question into where the instruction goes in here. And then it's gonna give us our answer out. Our answer out is gonna be the capital of\\nEngland is London. And then if I ask it, the typical sort of\\nalpaca question of, okay, what are alpacas and how are they different from llamas? You can see here we get, the standard answer\\nthat we got when we just [00:03:00] played around with, the model by itself without LangChain. All right, so setting up the conversation\\npart is one of the key things is we want to make use of the memory here. So we are going to Lama and alpaca have a\\ngood sized token span. So we can actually go up to sort of 2000 tokens\\nhere. What we're doing now, unfortunately, we fine\\ntuned it probably for a lot less than that. So it will be interesting to see. And I would like to hear back from you guys\\nas well. How well does it do when you get a really\\nlong token span in there? . It may do really good. You might find it at times that it doesn't\\ndo as good but okay. So we're gonna set up our conversation chain. So if you remember, one of the things that\\nwe used in a conversation chain is the whole idea of a memory. And the particular memory that I've chosen\\nto use here is this conversation, buffer window memory. So what this is gonna do is give us a window\\nthat we pass across the conversation. [00:04:00] and we are going to represent K\\nnumber of turns. . So for example, in this case, I've decided\\nto set K to four, which means we're gonna have four turns going on. And that will be the limit. So this should keep us so our token span never\\ngets too wide in the conversation. All right, setting up the conversation chain. We just passed in our local LLM that we set\\nup earlier with the hugging face pipeline. We're gonna pass in our memory to be the,\\nthis window memory that we've created. And we're gonna just set verbose equals true\\nso we can just see what's going on. If we look at the the template here, So just\\nto see what the template, the standard template is it looks something like this. The following is a friendly conversation between\\na human and an ai. The AI is talkative and provides lots of specific\\ndetails from its context. If the AI does not know the answer, it truthfully\\nsays it does not know. And so you can see in this prompt we're injecting\\ntwo things. We're injecting the human input and we're\\nalso injecting the [00:05:00] history or the memory. That's coming of the conversation as it's\\ngone through. So I decided to modify this a little bit. So here's the version that I modified it to. And you obviously play around with this yourself. The following is a friendly conversation between\\na human and an AI called alpaca. So we wanted to have a little bit of knowledge\\nabout who it is. So we could also put some things in there\\nthat, Alpaca is three years old. Alpaca loves to eat apples. You could put some other things in there too. I find it's always fun. To watch people play with this if you've set\\nit up to be quite funny as well, so you could try that out. so to do that, we basically just override\\nthe conversation dot prompt dot template. So you can see I'm just taking that new conversation\\nprompt template and trying it out. . so now it's got our name being alpaca and\\nstuff in there. All right, now we get to talking to it. the first thing that I tried, Al, was basically\\njust saying, hi, there I am Sam. . So it's interesting here. we know that the alpaca is [00:06:00] fine\\ntuned, not on dialogue, right? We haven't done a version that's fine tuned\\non dialogue yet. Maybe that will be a future video. But here we've done it's fine tuned on tasks,\\nso they tend to be tasks where you ask it. It to give you a fact, or you ask it to reproduce\\na list of something. That kind of thing. So when I don't actually ask it something,\\nit doesn't do a great job. It's perhaps a little bit confused in here. And what does it do? It doesn't generate bad text. It just then goes on to generate more text\\nthan we asked for. So it's taken in, you know that oh, it's talking\\nto Sam. Hey there, Sam, it's nice to meet you. What can I help you with? And then it generates Sam's response back\\nof, do you know what time it is, and then alpaca. Sure. Sure do. It's currently, and then it obviously passing\\nin a token for where you could substitute the time. You could do a regex change or something there\\nand put in the current time. . So then I realized that, okay, this is probably\\nhappening because we [00:07:00] didn't ask it a question. So it's trying to just keep being chatty and\\nmake up things itself. So if we try it and we ask it, okay, what\\nis your name? So now it's much more on point, right? It's okay, my name is Alpaca. How may I help you today? . So we've now got multiple turns in here,\\nbut we probably shouldn't count the turn where it generated. It's Sam and it's an alpaca. It's really should be. Human AI is one turn in there. now, I ask it another question. Can you tell me what an alpaca is? Sure. It's gives us an answer. And it's not too long here, you can see that\\nan alpaca is a species of a South American camelid mammal. They're typically brown or white in color\\nand have long necks and legs, All right. So then I ask how is it different than a lama? So you see at this point now we've got these\\nmultiple steps of our memory being passed in, of the conversation. So we've got all the steps from the start\\nbeing passed in here cuz we haven't met the sort of length of the window where it needs\\nto [00:08:00] start cutting things off yet. Okay. How is it different than a llama? Alpacas and llamas are both members of the\\ncamelaide family? I'm not sure how you pronounce that. But they differ in several ways. Alpacas tend to be smaller than llamas and\\nthey're, and it then it stops. so it should have been able to go on not ideal\\nagain, remember, this hasn't been fine tuned for conversation. So we are just using the fine tuned version\\nof Alpaca. can you gimme some good names for a pet Lama? Now if we go back to here, look at the memory. So this is the next thing I'm gonna ask it. You can see we've lost the start of the conversation. So where I said, hi, my name is Sam. You know that's all been taken out now. We're going as far back now as where I asked,\\nwhat is your name? So we've got one human, two human, three human,\\nfour human and then this being passed in. so our memory is a window of four that we've\\ngot going on there. Now [00:09:00] you could experiment with a\\nlonger memory if you want to. But remember all this is going Into the language\\nmodel. So you will find that if your memory gets\\ntoo long, then it can actually be, quite slow to do things. Alright. So I ask it, can you give me some good names\\nfor a pet lama? Sure. Here are some great options. Pata, Tika, cushy and Wayra. Okay. And then I wanted to test it, see, does it\\nstill remember its name? So I ask you, is your name Fred? And you can see that we've lost The bit in\\nthe conversation where it told us our name. So it's only relying on the name being up\\nthere. Yet still it understands from the context. No. My name is Alpaca. So that's good. We next asked it. Okay, what food should I feed my new llama? So again, you see where our window of four\\nis staying fixed, so we are losing things that we spoke about early on in the conversation. Okay. And then finally, your new llama can eat grass,\\nhay even alfalfa. You can also try giving them some [00:10:00]\\nvegetables like carrots, apples, and banana. . So this is just setting it up now. You could actually try and mess with the prompt\\nmore to basically inject the whole personality in there. Do some things like that. But the idea here is that it turns out that\\nllama's doing pretty well. I wouldn't say great, but it's doing pretty\\nwell with the prompt for the chatbot and then passing in this memory that we've got going\\non the current conversation. And then generating out. So this is something you could try an experiment\\nwith and this would also work on the llama model if you want to actually try the llama\\nmodel. My guess is that probably won't do as well\\nas the alpaca model. I don't know, maybe I will try that out and\\nwe can look at that in another video. Anyway, as always, if you have any questions\\nplease put them in the. And if this was useful to you please click\\nand subscribe. I will see you in the next video. Bye for now.\"}]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The video demonstrates how to hook up Alpaca, a language model, to LangChain for a chatbot application. The process involves installing Transformers from GitHub, bringing in the Hugging Face Pipeline, and setting up a local LLM from the pipeline. The conversation chain is set up using a memory window, and the model is tested with various prompts and questions. The video concludes by suggesting that the same process can be applied to the Llama model, although it may not perform as well as Alpaca.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quiz Generator based on context"
      ],
      "metadata": {
        "id": "4jGcDgtEQ31j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Create a 10 question multiple choice quiz based on the video transcript specified below and provide solutions.\n",
        "TRANSCRIPT:\n",
        "<<content>>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MH6j1XVmRHYd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt.replace(\"<<content>>\",documents[0].page_content)"
      ],
      "metadata": {
        "id": "HScFs2UIRY2U"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_function(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFzxuU1qRcnk",
        "outputId": "2422a97c-83c6-4f1a-a3cb-6d7fdc1a9632"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'Your are a helpful assistant.'}, {'role': 'user', 'content': \"Create a 10 question multiple choice quiz based on the video transcript specified below and provide solutions.\\nTRANSCRIPT:\\n[00:00:00] Okay. So a lot of people have been asking me how\\ncould we hook up alpaca to LangChain and try it out for a chat bot kind of thing. You can do this quite easy. I'm gonna show you a way in this video, and\\nthen later on in another video I'll show you some ways that we could take what we fine\\ntuned our special version and use that as well. So you're going to need to install Transformers\\nfrom the main branch, from GitHub. If you just install the normal pip version\\nyou won't get the version with the right tokenizer and the right model for doing this. So you can basically just bring that in. You're also gonna need bits and bytes. If you're gonna do an eight bit version and\\nyou of course, you're gonna need lang chain. so what we're gonna be doing is we're gonna\\nbe using the hugging face LLM wrapper in LangChain to run this so you can see that. Okay. First off, we are just gonna be bringing in\\nour llama, tokenizer and Lama, token and lama for causal language model. remember that alpaca is built on llama and\\nis made for a model that's the [00:01:00] similar style that as our alpaca is. So that's why we are using theseFrom LangChain. We're gonna be bringing in the Hugging Face\\nPipeline . We're also gonna need to just bring in, some other things as we go. So first off, I'm bringing in the template\\nand the LM chain, just to show you the, okay. We can bring this in. We can set up, our model. We've bringing it in as eight bits so that\\nwe're gonna use less memory and it'll be faster for inference. . And here is where a lot of the magic happens\\nis that the way that the Lama model and alpaca model are set up we can actually set up a\\nhugging face pipeline for them, which is gonna be doing text generation. And we just then just pass in this model into\\nhere. We pass in the tokenizer, we pass in things\\nlike our max length, our temperature our, top p and the repetition penalty in here as\\nwell. and then we can set up a local. LLM from this hugging face pipeline. So this is actually, coming from LangChain\\nand it's allowing us to set [00:02:00] up a LLM with this hugging face pipeline there. Once we've got that done, we can basically\\ntry it out with just a normal LLM chain. So I'm just gonna take one of the standard\\nalpaca prompts. And style of prompts for the template. So we create a prompt template in here. We pass this in and then you can see we're\\nbasically just setting up our prompt template with the template that we've made here. And then the instruction is just gonna be\\nthe variable that we're gonna pass in. So this is standard stuff. We've then basically got this going on here\\nso we can have, what is the capital of England? And sure enough this is just going to inject\\nthis question into where the instruction goes in here. And then it's gonna give us our answer out. Our answer out is gonna be the capital of\\nEngland is London. And then if I ask it, the typical sort of\\nalpaca question of, okay, what are alpacas and how are they different from llamas? You can see here we get, the standard answer\\nthat we got when we just [00:03:00] played around with, the model by itself without LangChain. All right, so setting up the conversation\\npart is one of the key things is we want to make use of the memory here. So we are going to Lama and alpaca have a\\ngood sized token span. So we can actually go up to sort of 2000 tokens\\nhere. What we're doing now, unfortunately, we fine\\ntuned it probably for a lot less than that. So it will be interesting to see. And I would like to hear back from you guys\\nas well. How well does it do when you get a really\\nlong token span in there? . It may do really good. You might find it at times that it doesn't\\ndo as good but okay. So we're gonna set up our conversation chain. So if you remember, one of the things that\\nwe used in a conversation chain is the whole idea of a memory. And the particular memory that I've chosen\\nto use here is this conversation, buffer window memory. So what this is gonna do is give us a window\\nthat we pass across the conversation. [00:04:00] and we are going to represent K\\nnumber of turns. . So for example, in this case, I've decided\\nto set K to four, which means we're gonna have four turns going on. And that will be the limit. So this should keep us so our token span never\\ngets too wide in the conversation. All right, setting up the conversation chain. We just passed in our local LLM that we set\\nup earlier with the hugging face pipeline. We're gonna pass in our memory to be the,\\nthis window memory that we've created. And we're gonna just set verbose equals true\\nso we can just see what's going on. If we look at the the template here, So just\\nto see what the template, the standard template is it looks something like this. The following is a friendly conversation between\\na human and an ai. The AI is talkative and provides lots of specific\\ndetails from its context. If the AI does not know the answer, it truthfully\\nsays it does not know. And so you can see in this prompt we're injecting\\ntwo things. We're injecting the human input and we're\\nalso injecting the [00:05:00] history or the memory. That's coming of the conversation as it's\\ngone through. So I decided to modify this a little bit. So here's the version that I modified it to. And you obviously play around with this yourself. The following is a friendly conversation between\\na human and an AI called alpaca. So we wanted to have a little bit of knowledge\\nabout who it is. So we could also put some things in there\\nthat, Alpaca is three years old. Alpaca loves to eat apples. You could put some other things in there too. I find it's always fun. To watch people play with this if you've set\\nit up to be quite funny as well, so you could try that out. so to do that, we basically just override\\nthe conversation dot prompt dot template. So you can see I'm just taking that new conversation\\nprompt template and trying it out. . so now it's got our name being alpaca and\\nstuff in there. All right, now we get to talking to it. the first thing that I tried, Al, was basically\\njust saying, hi, there I am Sam. . So it's interesting here. we know that the alpaca is [00:06:00] fine\\ntuned, not on dialogue, right? We haven't done a version that's fine tuned\\non dialogue yet. Maybe that will be a future video. But here we've done it's fine tuned on tasks,\\nso they tend to be tasks where you ask it. It to give you a fact, or you ask it to reproduce\\na list of something. That kind of thing. So when I don't actually ask it something,\\nit doesn't do a great job. It's perhaps a little bit confused in here. And what does it do? It doesn't generate bad text. It just then goes on to generate more text\\nthan we asked for. So it's taken in, you know that oh, it's talking\\nto Sam. Hey there, Sam, it's nice to meet you. What can I help you with? And then it generates Sam's response back\\nof, do you know what time it is, and then alpaca. Sure. Sure do. It's currently, and then it obviously passing\\nin a token for where you could substitute the time. You could do a regex change or something there\\nand put in the current time. . So then I realized that, okay, this is probably\\nhappening because we [00:07:00] didn't ask it a question. So it's trying to just keep being chatty and\\nmake up things itself. So if we try it and we ask it, okay, what\\nis your name? So now it's much more on point, right? It's okay, my name is Alpaca. How may I help you today? . So we've now got multiple turns in here,\\nbut we probably shouldn't count the turn where it generated. It's Sam and it's an alpaca. It's really should be. Human AI is one turn in there. now, I ask it another question. Can you tell me what an alpaca is? Sure. It's gives us an answer. And it's not too long here, you can see that\\nan alpaca is a species of a South American camelid mammal. They're typically brown or white in color\\nand have long necks and legs, All right. So then I ask how is it different than a lama? So you see at this point now we've got these\\nmultiple steps of our memory being passed in, of the conversation. So we've got all the steps from the start\\nbeing passed in here cuz we haven't met the sort of length of the window where it needs\\nto [00:08:00] start cutting things off yet. Okay. How is it different than a llama? Alpacas and llamas are both members of the\\ncamelaide family? I'm not sure how you pronounce that. But they differ in several ways. Alpacas tend to be smaller than llamas and\\nthey're, and it then it stops. so it should have been able to go on not ideal\\nagain, remember, this hasn't been fine tuned for conversation. So we are just using the fine tuned version\\nof Alpaca. can you gimme some good names for a pet Lama? Now if we go back to here, look at the memory. So this is the next thing I'm gonna ask it. You can see we've lost the start of the conversation. So where I said, hi, my name is Sam. You know that's all been taken out now. We're going as far back now as where I asked,\\nwhat is your name? So we've got one human, two human, three human,\\nfour human and then this being passed in. so our memory is a window of four that we've\\ngot going on there. Now [00:09:00] you could experiment with a\\nlonger memory if you want to. But remember all this is going Into the language\\nmodel. So you will find that if your memory gets\\ntoo long, then it can actually be, quite slow to do things. Alright. So I ask it, can you give me some good names\\nfor a pet lama? Sure. Here are some great options. Pata, Tika, cushy and Wayra. Okay. And then I wanted to test it, see, does it\\nstill remember its name? So I ask you, is your name Fred? And you can see that we've lost The bit in\\nthe conversation where it told us our name. So it's only relying on the name being up\\nthere. Yet still it understands from the context. No. My name is Alpaca. So that's good. We next asked it. Okay, what food should I feed my new llama? So again, you see where our window of four\\nis staying fixed, so we are losing things that we spoke about early on in the conversation. Okay. And then finally, your new llama can eat grass,\\nhay even alfalfa. You can also try giving them some [00:10:00]\\nvegetables like carrots, apples, and banana. . So this is just setting it up now. You could actually try and mess with the prompt\\nmore to basically inject the whole personality in there. Do some things like that. But the idea here is that it turns out that\\nllama's doing pretty well. I wouldn't say great, but it's doing pretty\\nwell with the prompt for the chatbot and then passing in this memory that we've got going\\non the current conversation. And then generating out. So this is something you could try an experiment\\nwith and this would also work on the llama model if you want to actually try the llama\\nmodel. My guess is that probably won't do as well\\nas the alpaca model. I don't know, maybe I will try that out and\\nwe can look at that in another video. Anyway, as always, if you have any questions\\nplease put them in the. And if this was useful to you please click\\nand subscribe. I will see you in the next video. Bye for now.\\n\"}]\n",
            "1. What is the purpose of the video?\n",
            "A. To show how to hook up alpaca to LangChain for a chatbot\n",
            "B. To show how to install Transformers from GitHub\n",
            "C. To show how to use the Hugging Face Pipeline\n",
            "D. To show how to set up a LLM with LangChain\n",
            "\n",
            "Answer: A\n",
            "\n",
            "2. Why do you need to install Transformers from GitHub?\n",
            "A. To get the right tokenizer and model for doing this\n",
            "B. To get the normal pip version\n",
            "C. To get the right model for doing this\n",
            "D. To get the right tokenizer for doing this\n",
            "\n",
            "Answer: A\n",
            "\n",
            "3. What do you need to install if you're going to do an eight-bit version?\n",
            "A. LangChain\n",
            "B. Transformers\n",
            "C. Bits and bytes\n",
            "D. Hugging Face Pipeline\n",
            "\n",
            "Answer: C\n",
            "\n",
            "4. What is the purpose of the Hugging Face LLM wrapper in LangChain?\n",
            "A. To run the model\n",
            "B. To set up the model\n",
            "C. To fine-tune the model\n",
            "D. To generate text\n",
            "\n",
            "Answer: A\n",
            "\n",
            "5. What is the purpose of the conversation buffer window memory?\n",
            "A. To give us a window that we pass across the conversation\n",
            "B. To represent K number of turns\n",
            "C. To keep the token span from getting too wide in the conversation\n",
            "D. All of the above\n",
            "\n",
            "Answer: D\n",
            "\n",
            "6. What is the standard template for the conversation prompt?\n",
            "A. The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer, it truthfully says it does not know.\n",
            "B. The following is a friendly conversation between a human and an AI called alpaca.\n",
            "C. The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n",
            "D. The following is a friendly conversation between a human and an AI. If the AI does not know the answer, it truthfully says it does not know.\n",
            "\n",
            "Answer: A\n",
            "\n",
            "7. What is the purpose of the memory being passed in during the conversation?\n",
            "A. To keep track of the conversation history\n",
            "B. To help the model generate more accurate responses\n",
            "C. To make the conversation more interesting\n",
            "D. To make the conversation longer\n",
            "\n",
            "Answer: A\n",
            "\n",
            "8. What is the purpose of the conversation chain?\n",
            "A. To set up the model\n",
            "B. To fine-tune the model\n",
            "C. To generate text\n",
            "D. To pass in the memory and generate responses\n",
            "\n",
            "Answer: D\n",
            "\n",
            "9. What is the purpose of the modified conversation prompt template?\n",
            "A. To inject some personality into the conversation\n",
            "B. To make the conversation more interesting\n",
            "C. To give some knowledge about who the AI is\n",
            "D. All of the above\n",
            "\n",
            "Answer: D\n",
            "\n",
            "10. What is the conclusion of the video?\n",
            "A. Alpaca is doing pretty well with the prompt for the chatbot and passing in the current conversation memory\n",
            "B. Llama is doing pretty well with the prompt for the chatbot and passing in the current conversation memory\n",
            "C. Alpaca and Llama are both doing well with the prompt for the chatbot and passing in the current conversation memory\n",
            "D. None of the above\n",
            "\n",
            "Answer: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Each duration Summary generation"
      ],
      "metadata": {
        "id": "_3Ut0QS9TJBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Create an extremly clear and concise summary for each duration specified in the video transcript specified below and provide solutions.The output\n",
        "should be in the following format:\n",
        "duration : summary \n",
        "TRANSCRIPT:\n",
        "<<content>>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kTz0lHoRRvhN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt.replace(\"<<content>>\",documents[0].page_content)"
      ],
      "metadata": {
        "id": "Ah3rRkAWSWxb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_function(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrqWS2N6ScDT",
        "outputId": "f7951bb5-4ba2-41a3-eb25-98b7d597b84b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'Your are a helpful assistant.'}, {'role': 'user', 'content': \"Create an extremly clear and concise summary for each duration specified in the video transcript specified below and provide solutions.The output\\nshould be in the following format:\\nduration : summary \\nTRANSCRIPT:\\n[00:00:00] Okay. So a lot of people have been asking me how\\ncould we hook up alpaca to LangChain and try it out for a chat bot kind of thing. You can do this quite easy. I'm gonna show you a way in this video, and\\nthen later on in another video I'll show you some ways that we could take what we fine\\ntuned our special version and use that as well. So you're going to need to install Transformers\\nfrom the main branch, from GitHub. If you just install the normal pip version\\nyou won't get the version with the right tokenizer and the right model for doing this. So you can basically just bring that in. You're also gonna need bits and bytes. If you're gonna do an eight bit version and\\nyou of course, you're gonna need lang chain. so what we're gonna be doing is we're gonna\\nbe using the hugging face LLM wrapper in LangChain to run this so you can see that. Okay. First off, we are just gonna be bringing in\\nour llama, tokenizer and Lama, token and lama for causal language model. remember that alpaca is built on llama and\\nis made for a model that's the [00:01:00] similar style that as our alpaca is. So that's why we are using theseFrom LangChain. We're gonna be bringing in the Hugging Face\\nPipeline . We're also gonna need to just bring in, some other things as we go. So first off, I'm bringing in the template\\nand the LM chain, just to show you the, okay. We can bring this in. We can set up, our model. We've bringing it in as eight bits so that\\nwe're gonna use less memory and it'll be faster for inference. . And here is where a lot of the magic happens\\nis that the way that the Lama model and alpaca model are set up we can actually set up a\\nhugging face pipeline for them, which is gonna be doing text generation. And we just then just pass in this model into\\nhere. We pass in the tokenizer, we pass in things\\nlike our max length, our temperature our, top p and the repetition penalty in here as\\nwell. and then we can set up a local. LLM from this hugging face pipeline. So this is actually, coming from LangChain\\nand it's allowing us to set [00:02:00] up a LLM with this hugging face pipeline there. Once we've got that done, we can basically\\ntry it out with just a normal LLM chain. So I'm just gonna take one of the standard\\nalpaca prompts. And style of prompts for the template. So we create a prompt template in here. We pass this in and then you can see we're\\nbasically just setting up our prompt template with the template that we've made here. And then the instruction is just gonna be\\nthe variable that we're gonna pass in. So this is standard stuff. We've then basically got this going on here\\nso we can have, what is the capital of England? And sure enough this is just going to inject\\nthis question into where the instruction goes in here. And then it's gonna give us our answer out. Our answer out is gonna be the capital of\\nEngland is London. And then if I ask it, the typical sort of\\nalpaca question of, okay, what are alpacas and how are they different from llamas? You can see here we get, the standard answer\\nthat we got when we just [00:03:00] played around with, the model by itself without LangChain. All right, so setting up the conversation\\npart is one of the key things is we want to make use of the memory here. So we are going to Lama and alpaca have a\\ngood sized token span. So we can actually go up to sort of 2000 tokens\\nhere. What we're doing now, unfortunately, we fine\\ntuned it probably for a lot less than that. So it will be interesting to see. And I would like to hear back from you guys\\nas well. How well does it do when you get a really\\nlong token span in there? . It may do really good. You might find it at times that it doesn't\\ndo as good but okay. So we're gonna set up our conversation chain. So if you remember, one of the things that\\nwe used in a conversation chain is the whole idea of a memory. And the particular memory that I've chosen\\nto use here is this conversation, buffer window memory. So what this is gonna do is give us a window\\nthat we pass across the conversation. [00:04:00] and we are going to represent K\\nnumber of turns. . So for example, in this case, I've decided\\nto set K to four, which means we're gonna have four turns going on. And that will be the limit. So this should keep us so our token span never\\ngets too wide in the conversation. All right, setting up the conversation chain. We just passed in our local LLM that we set\\nup earlier with the hugging face pipeline. We're gonna pass in our memory to be the,\\nthis window memory that we've created. And we're gonna just set verbose equals true\\nso we can just see what's going on. If we look at the the template here, So just\\nto see what the template, the standard template is it looks something like this. The following is a friendly conversation between\\na human and an ai. The AI is talkative and provides lots of specific\\ndetails from its context. If the AI does not know the answer, it truthfully\\nsays it does not know. And so you can see in this prompt we're injecting\\ntwo things. We're injecting the human input and we're\\nalso injecting the [00:05:00] history or the memory. That's coming of the conversation as it's\\ngone through. So I decided to modify this a little bit. So here's the version that I modified it to. And you obviously play around with this yourself. The following is a friendly conversation between\\na human and an AI called alpaca. So we wanted to have a little bit of knowledge\\nabout who it is. So we could also put some things in there\\nthat, Alpaca is three years old. Alpaca loves to eat apples. You could put some other things in there too. I find it's always fun. To watch people play with this if you've set\\nit up to be quite funny as well, so you could try that out. so to do that, we basically just override\\nthe conversation dot prompt dot template. So you can see I'm just taking that new conversation\\nprompt template and trying it out. . so now it's got our name being alpaca and\\nstuff in there. All right, now we get to talking to it. the first thing that I tried, Al, was basically\\njust saying, hi, there I am Sam. . So it's interesting here. we know that the alpaca is [00:06:00] fine\\ntuned, not on dialogue, right? We haven't done a version that's fine tuned\\non dialogue yet. Maybe that will be a future video. But here we've done it's fine tuned on tasks,\\nso they tend to be tasks where you ask it. It to give you a fact, or you ask it to reproduce\\na list of something. That kind of thing. So when I don't actually ask it something,\\nit doesn't do a great job. It's perhaps a little bit confused in here. And what does it do? It doesn't generate bad text. It just then goes on to generate more text\\nthan we asked for. So it's taken in, you know that oh, it's talking\\nto Sam. Hey there, Sam, it's nice to meet you. What can I help you with? And then it generates Sam's response back\\nof, do you know what time it is, and then alpaca. Sure. Sure do. It's currently, and then it obviously passing\\nin a token for where you could substitute the time. You could do a regex change or something there\\nand put in the current time. . So then I realized that, okay, this is probably\\nhappening because we [00:07:00] didn't ask it a question. So it's trying to just keep being chatty and\\nmake up things itself. So if we try it and we ask it, okay, what\\nis your name? So now it's much more on point, right? It's okay, my name is Alpaca. How may I help you today? . So we've now got multiple turns in here,\\nbut we probably shouldn't count the turn where it generated. It's Sam and it's an alpaca. It's really should be. Human AI is one turn in there. now, I ask it another question. Can you tell me what an alpaca is? Sure. It's gives us an answer. And it's not too long here, you can see that\\nan alpaca is a species of a South American camelid mammal. They're typically brown or white in color\\nand have long necks and legs, All right. So then I ask how is it different than a lama? So you see at this point now we've got these\\nmultiple steps of our memory being passed in, of the conversation. So we've got all the steps from the start\\nbeing passed in here cuz we haven't met the sort of length of the window where it needs\\nto [00:08:00] start cutting things off yet. Okay. How is it different than a llama? Alpacas and llamas are both members of the\\ncamelaide family? I'm not sure how you pronounce that. But they differ in several ways. Alpacas tend to be smaller than llamas and\\nthey're, and it then it stops. so it should have been able to go on not ideal\\nagain, remember, this hasn't been fine tuned for conversation. So we are just using the fine tuned version\\nof Alpaca. can you gimme some good names for a pet Lama? Now if we go back to here, look at the memory. So this is the next thing I'm gonna ask it. You can see we've lost the start of the conversation. So where I said, hi, my name is Sam. You know that's all been taken out now. We're going as far back now as where I asked,\\nwhat is your name? So we've got one human, two human, three human,\\nfour human and then this being passed in. so our memory is a window of four that we've\\ngot going on there. Now [00:09:00] you could experiment with a\\nlonger memory if you want to. But remember all this is going Into the language\\nmodel. So you will find that if your memory gets\\ntoo long, then it can actually be, quite slow to do things. Alright. So I ask it, can you give me some good names\\nfor a pet lama? Sure. Here are some great options. Pata, Tika, cushy and Wayra. Okay. And then I wanted to test it, see, does it\\nstill remember its name? So I ask you, is your name Fred? And you can see that we've lost The bit in\\nthe conversation where it told us our name. So it's only relying on the name being up\\nthere. Yet still it understands from the context. No. My name is Alpaca. So that's good. We next asked it. Okay, what food should I feed my new llama? So again, you see where our window of four\\nis staying fixed, so we are losing things that we spoke about early on in the conversation. Okay. And then finally, your new llama can eat grass,\\nhay even alfalfa. You can also try giving them some [00:10:00]\\nvegetables like carrots, apples, and banana. . So this is just setting it up now. You could actually try and mess with the prompt\\nmore to basically inject the whole personality in there. Do some things like that. But the idea here is that it turns out that\\nllama's doing pretty well. I wouldn't say great, but it's doing pretty\\nwell with the prompt for the chatbot and then passing in this memory that we've got going\\non the current conversation. And then generating out. So this is something you could try an experiment\\nwith and this would also work on the llama model if you want to actually try the llama\\nmodel. My guess is that probably won't do as well\\nas the alpaca model. I don't know, maybe I will try that out and\\nwe can look at that in another video. Anyway, as always, if you have any questions\\nplease put them in the. And if this was useful to you please click\\nand subscribe. I will see you in the next video. Bye for now.\\n\"}]\n",
            "00:00:00 - Introduction and installation of necessary packages for setting up a chatbot using Alpaca and LangChain.\n",
            "\n",
            "00:01:00 - Setting up the Hugging Face pipeline for text generation using the LLM wrapper in LangChain.\n",
            "\n",
            "00:02:00 - Setting up a local LLM from the Hugging Face pipeline and trying it out with a standard Alpaca prompt.\n",
            "\n",
            "00:03:00 - Setting up a conversation chain using a memory window to keep track of the conversation history.\n",
            "\n",
            "00:04:00 - Modifying the conversation prompt template to include information about the AI and injecting the human input and conversation history.\n",
            "\n",
            "00:05:00 - Testing the chatbot by initiating a conversation with a greeting and asking questions.\n",
            "\n",
            "00:06:00 - Observing the chatbot's response when not asked a question and adjusting the conversation accordingly.\n",
            "\n",
            "00:07:00 - Asking the chatbot questions about Alpaca and Llama and observing its response.\n",
            "\n",
            "00:08:00 - Testing the chatbot's memory by asking a question about pet Llamas and observing the conversation history.\n",
            "\n",
            "00:09:00 - Continuing the conversation and observing the chatbot's ability to remember its name and provide information about feeding Llamas.\n",
            "\n",
            "00:10:00 - Conclusion and suggestions for further experimentation with the chatbot.\n"
          ]
        }
      ]
    }
  ]
}